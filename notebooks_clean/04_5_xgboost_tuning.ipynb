{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f77990",
   "metadata": {},
   "source": [
    "# 04.5 - Dedicated XGBoost Tuning\n",
    "\n",
    "This notebook isolates XGBoost tuning. It contains: the original XGBoost tuning cell (kept as a reference) and an improved, safer tuning cell designed to avoid long timeouts and failures while still searching useful hyperparameter space.\n",
    "\n",
    "Run this on an interactive node (Amarel) with a conda environment that has papermill/jupyter installed. Save artifacts land in `models/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fc530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "print(f\"Notebook run: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data (portable path)\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "X_train = pd.read_csv(DATA_DIR / 'X_train.csv')\n",
    "X_test = pd.read_csv(DATA_DIR / 'X_test.csv')\n",
    "y_train = pd.read_csv(DATA_DIR / 'y_train.csv')['ClosePrice'].values\n",
    "y_test = pd.read_csv(DATA_DIR / 'y_test.csv')['ClosePrice'].values\n",
    "\n",
    "# Sanitize column names for tree-based libraries (LightGBM/XGBoost)\n",
    "import re\n",
    "def _clean_col(c):\n",
    "    return re.sub(r'[^0-9a-zA-Z_]', '_', str(c))\n",
    "\n",
    "orig_cols = list(X_train.columns)\n",
    "new_cols = [_clean_col(c) for c in orig_cols]\n",
    "if new_cols != orig_cols:\n",
    "    print('Sanitizing feature names: replacing special characters with underscores')\n",
    "    X_train.columns = new_cols\n",
    "    X_test.columns = [_clean_col(c) for c in X_test.columns]\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Testing: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function (copied from main notebook)\n",
    "import time\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "        'test_mdape': np.median(np.abs((y_test - y_pred_test) / y_test)) * 100,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Train RÂ²: {results['train_r2']:.4f}\")\n",
    "    steph_threshold = 0.884\n",
    "    if results['test_r2'] > steph_threshold:\n",
    "        status = 'ðŸŽ‰ BEATS STEPH!'\n",
    "    else:\n",
    "        status = f\"(Gap: {(steph_threshold - results['test_r2']) * 100:.2f}% to Steph)\"\n",
    "    print(f\"Test RÂ²:  {results['test_r2']:.4f} {status}\")\n",
    "    print(f\"RMSE:     ${results['test_rmse']:,.0f}\")\n",
    "    print(f\"MAE:      ${results['test_mae']:,.0f}\")\n",
    "    print(f\"MdAPE:    {results['test_mdape']:.2f}%\")\n",
    "    print(f\"Time:     {train_time:.1f}s\")\n",
    "\n",
    "    return results, model\n",
    "\n",
    "# placeholders and container for results\n",
    "xgb_model = None\n",
    "advanced_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da850c40",
   "metadata": {},
   "source": [
    "## Original XGBoost tuning cell (reference)\n",
    "\n",
    "The cell below is copied verbatim from `04_advanced_models_tuning.ipynb` so you can compare behavior and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165043a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning XGBoost (current best: 83.91%)...\\n\")\n",
    "\n",
    "xgb_model = None\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import traceback\n",
    "    import gc\n",
    "    import numpy as _np\n",
    "\n",
    "    xgb_param_dist = {\n",
    "        'n_estimators': [200, 300, 500],\n",
    "        'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
    "        'max_depth': [5, 7, 9, 11],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'gamma': [0, 0.1, 0.2],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [0.5, 1, 1.5, 2]\n",
    "    }\n",
    "\n",
    "    # Set estimator n_jobs=1 to avoid nested threading with RandomizedSearchCV outer parallelism\n",
    "    xgb_search = RandomizedSearchCV(\n",
    "        xgb.XGBRegressor(random_state=42, tree_method='hist', n_jobs=1, use_label_encoder=False, verbosity=0),\n",
    "        param_distributions=xgb_param_dist,\n",
    "        n_iter=80,  # keep or reduce for faster dev runs\n",
    "        cv=3,  # keep 3 for now; switch to 5 for final runs\n",
    "        scoring='r2',\n",
    "        random_state=42,\n",
    "        verbose=2,\n",
    "        n_jobs=4,  # outer parallelism\n",
    "        error_score=float('nan')\n",
    "    )\n",
    "\n",
    "    # Run the search WITHOUT passing eval_set / early stopping (to avoid leakage & CV weirdness)\n",
    "    try:\n",
    "        xgb_search.fit(X_train, y_train)\n",
    "    except Exception as fit_exc:\n",
    "        print('RandomizedSearchCV.fit raised an exception during XGBoost search:')\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    # If the search succeeded but produced no valid best estimator, fall back to a smaller, safer search\n",
    "    best = None\n",
    "    best_score = getattr(xgb_search, 'best_score_', _np.nan)\n",
    "    if hasattr(xgb_search, 'best_estimator_') and xgb_search.best_estimator_ is not None and not _np.isnan(best_score):\n",
    "        best = xgb_search.best_estimator_\n",
    "        print(f\"\\nBest XGBoost params: {xgb_search.best_params_}\")\n",
    "        print(f\"Best CV RÂ²: {xgb_search.best_score_:.4f}\")\n",
    "    else:\n",
    "        print('\\nXGBoost search did not return a valid best estimator (NaN or exception). Trying a small fallback search...')\n",
    "        # small, conservative fallback grid to avoid rare failure cases or bad param combos\n",
    "        try:\n",
    "            fallback_dist = {\n",
    "                'n_estimators': [200, 300],\n",
    "                'learning_rate': [0.03, 0.05],\n",
    "                'max_depth': [5, 7],\n",
    "                'min_child_weight': [1, 3],\n",
    "                'subsample': [0.8, 0.9],\n",
    "                'colsample_bytree': [0.8, 0.9],\n",
    "                'reg_alpha': [0, 0.1],\n",
    "                'reg_lambda': [0.5, 1]\n",
    "            }\n",
    "            fallback_search = RandomizedSearchCV(\n",
    "                xgb.XGBRegressor(random_state=42, tree_method='hist', n_jobs=1, use_label_encoder=False, verbosity=0),\n",
    "                param_distributions=fallback_dist,\n",
    "                n_iter=10,\n",
    "                cv=3,\n",
    "                scoring='r2',\n",
    "                random_state=42,\n",
    "                verbose=2,\n",
    "                n_jobs=2,\n",
    "                error_score=float('nan')\n",
    "            )\n",
    "            fallback_search.fit(X_train, y_train)\n",
    "            if hasattr(fallback_search, 'best_estimator_') and fallback_search.best_estimator_ is not None and not _np.isnan(getattr(fallback_search, 'best_score_', _np.nan)):\n",
    "                best = fallback_search.best_estimator_\n",
    "                print(f\"Fallback best params: {fallback_search.best_params_}\")\n",
    "                print(f\"Fallback CV RÂ²: {fallback_search.best_score_:.4f}\")\n",
    "            else:\n",
    "                print('Fallback search also failed or returned NaN. Will train a safe default XGBoost model.')\n",
    "        except Exception as fe:\n",
    "            print('Fallback search failed with exception:')\n",
    "            traceback.print_exc()\n",
    "        \n",
    "    if best is None:\n",
    "        # Final fallback: train a conservative default XGBoost model to keep pipeline moving\n",
    "        try:\n",
    "            print('\\nTraining default XGBoost (conservative settings) as last-resort fallback')\n",
    "            best = xgb.XGBRegressor(random_state=42, n_jobs=1, use_label_encoder=False, verbosity=0,\n",
    "                                     n_estimators=200, learning_rate=0.05, max_depth=7)\n",
    "            best.fit(X_train, y_train)\n",
    "        except Exception as final_exc:\n",
    "            print('Final fallback training also failed:')\n",
    "            traceback.print_exc()\n",
    "            raise final_exc\n",
    "\n",
    "    # Retrain best estimator with early stopping using a small validation split from the training set\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "    try:\n",
    "        best.set_params(n_jobs=1, use_label_encoder=False, verbosity=0)\n",
    "        best.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose=False)\n",
    "    except Exception as retrain_exc:\n",
    "        print('Retraining best estimator with early stopping failed; proceeding without early stopping for final fit:')\n",
    "        traceback.print_exc()\n",
    "        try:\n",
    "            best.fit(X_train, y_train)\n",
    "        except Exception as final_fit_exc:\n",
    "            print('Final best.fit failed:')\n",
    "            traceback.print_exc()\n",
    "            raise final_fit_exc\n",
    "\n",
    "    xgb_results, xgb_model = evaluate_model(best, X_train, X_test, y_train, y_test, \"XGBoost (Tuned)\")\n",
    "    advanced_results.append(xgb_results)\n",
    "\n",
    "    # cleanup\n",
    "    del xgb_search\n",
    "    gc.collect()\n",
    "\n",
    "except Exception as e:\n",
    "    print('XGBoost tuning failed (outer):', e)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    advanced_results.append({'model': 'XGBoost (failed)', 'test_r2': -999})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376495eb",
   "metadata": {},
   "source": [
    "## Improved XGBoost tuning cell (safer defaults)\n",
    "\n",
    "This cell uses a narrower search, fewer iterations, and robust fallbacks. `N_ITER` is configurable â€” reduce it during development to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32f094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved XGBoost tuning (safer)\n",
    "print(\"Running improved XGBoost tuning...\\n\")\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import traceback\n",
    "\n",
    "# Config: keep tuning tiny to avoid long runs / kernel problems on Amarel\n",
    "# Reduce N_ITER aggressively for a fast, safe run. Increase only on an interactive node when needed.\n",
    "N_ITER = 8  # << small, safe default for quick runs (use 30+ only on an interactive node)\n",
    "CV_FOLDS = 3  # keep small to save time; increase to 5 for final runs\n",
    "\n",
    "xgb_model = None\n",
    "try:\n",
    "    param_dist_safe = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.03, 0.05],\n",
    "        'max_depth': [5, 7, 9],\n",
    "        'min_child_weight': [1, 3],\n",
    "        'subsample': [0.7, 0.85, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "        'reg_alpha': [0, 0.1],\n",
    "        'reg_lambda': [0.5, 1.0]\n",
    "    }\n",
    "\n",
    "    # Estimator: force single-threaded training to avoid nested threading on cluster nodes\n",
    "    estimator = xgb.XGBRegressor(random_state=42, tree_method='hist', n_jobs=1, use_label_encoder=False, verbosity=0)\n",
    "    search = RandomizedSearchCV(\n",
    "    \"    estimator,\",\n",
    "    \"    param_distributions=param_dist_safe,\",\n",
    "    \"    n_iter=N_ITER,\",\n",
    "    \"    cv=CV_FOLDS,\",\n",
    "    \"    scoring='r2',\",\n",
    "    \"    random_state=42,\",\n",
    "    \"    verbose=0,  # mute noisy output in cluster runs\",\n",
    "    \"    n_jobs=1,  # avoid outer parallelism to prevent kernel crashes on shared nodes\",\n",
    "    \"    error_score=float('nan')\",\n",
    "    \")\",\"\",\n",
    "    \"try:\",\n",
    "    \"    search.fit(X_train, y_train)\",\n",
    "    \"except Exception as fit_exc:\",\n",
    "    \"    print('Search.fit raised an exception:')\",\n",
    "    \"    traceback.print_exc()\",\"\",\n",
    "    \"best = None\",\n",
    "    \"import numpy as _np\",\n",
    "    \"best_score = getattr(search, 'best_score_', _np.nan)\",\n",
    "    \"if hasattr(search, 'best_estimator_') and search.best_estimator_ is not None and not _np.isnan(best_score):\",\n",
    "    \"    best = search.best_estimator_\",\n",
    "    \"    print(f\\\"Found best params: {search.best_params_}\\\")\",\n",
    "    \"    print(f\\\"Best CV RÂ²: {search.best_score_:.4f}\\\")\",\n",
    "    \"else:\",\n",
    "    \"    print('Primary search did not return a usable best estimator (NaN or exception). Trying a small fallback search...')\",\n",
    "    \"    try:\",\n",
    "    \"        fallback = RandomizedSearchCV(\",\n",
    "    \"            estimator,\",\n",
    "    \"            param_distributions={\",\n",
    "    \"                'n_estimators': [100, 200],\",\n",
    "    \"                'learning_rate': [0.03, 0.05],\",\n",
    "    \"                'max_depth': [5, 7],\",\n",
    "    \"                'min_child_weight': [1, 3],\",\n",
    "    \"                'subsample': [0.8, 0.9],\",\n",
    "    \"                'colsample_bytree': [0.8, 0.9],\",\n",
    "    \"            },\",\n",
    "    \"            n_iter=6, cv=CV_FOLDS, scoring='r2', random_state=42, verbose=0, n_jobs=1, error_score=float('nan')\",\n",
    "    \"        )\",\n",
    "    \"        fallback.fit(X_train, y_train)\",\n",
    "    \"        if hasattr(fallback, 'best_estimator_') and fallback.best_estimator_ is not None and not _np.isnan(getattr(fallback, 'best_score_', _np.nan)):\",\n",
    "    \"            best = fallback.best_estimator_\",\n",
    "    \"            print(f\\\"Fallback best params: {fallback.best_params_}\\\")\",\n",
    "    \"            print(f\\\"Fallback CV RÂ²: {fallback.best_score_:.4f}\\\")\",\n",
    "    \"        else:\",\n",
    "    \"            print('Fallback search also failed or returned NaN. Training a conservative default XGBoost model.')\",\n",
    "    \"    except Exception as fe:\",\n",
    "    \"        print('Fallback failed:')\",\n",
    "    \"        traceback.print_exc()\",\"\",\n",
    "    \"if best is None:\",\n",
    "    \"    print('Training conservative default XGBoost as final fallback')\",\n",
    "    \"    best = xgb.XGBRegressor(random_state=42, n_jobs=1, use_label_encoder=False, verbosity=0, n_estimators=150, learning_rate=0.05, max_depth=7)\",\n",
    "    \"    best.fit(X_train, y_train)\",\n",
    "\",\n",
    "    \"# Retrain best with early stopping on a small validation split\",\n",
    "    \"X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\",\n",
    "    \"try:\",\n",
    "    \"    best.set_params(n_jobs=1, verbosity=0)\",\n",
    "    \"    best.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=30, verbose=False)\",\n",
    "    \"except Exception as retrain_exc:\",\n",
    "    \"    print('Retrain with early stopping failed, falling back to full-train:')\",\n",
    "    \"    traceback.print_exc()\",\n",
    "    \"    try:\",\n",
    "    \"        best.fit(X_train, y_train)\",\n",
    "    \"    except Exception as final_fit_exc:\",\n",
    "    \"        print('Final fit failed:')\",\n",
    "    \"        traceback.print_exc()\",\n",
    "    \"        raise final_fit_exc\",\"\",\n",
    "    \"# Evaluate and save\",\n",
    "    \"xgb_results, xgb_model = evaluate_model(best, X_train, X_test, y_train, y_test, 'XGBoost (Improved Tuned)')\",\n",
    "    \"advanced_results.append(xgb_results)\",\n",
    "\n",
    "    \"# Save model and results\",\n",
    "    \"joblib.dump(xgb_model, MODELS_DIR / 'best_xgb_tuned.joblib')\",\n",
    "    \"pd.DataFrame(advanced_results).to_csv(MODELS_DIR / 'xgb_tuning_results.csv', index=False)\",\n",
    "\n",
    "    \"# cleanup\",\n",
    "    \"del search\",\n",
    "    \"gc.collect()\",\"\",\n",
    "except Exception as e:\n",
    "    print('Improved XGBoost tuning cell failed:')\n",
    "    traceback.print_exc()\n",
    "    advanced_results.append({'model': 'XGBoost (failed)', 'test_r2': -999})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d30a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary print\n",
    "if len(advanced_results) > 0:\n",
    "    df = pd.DataFrame(advanced_results).sort_values('test_r2', ascending=False)\n",
    "    print('\\nXGBoost tuning summary:')\n",
    "    print(df.to_string(index=False))\n",
    "    print('Models saved to', MODELS_DIR)\n",
    "else:\n",
    "    print('No results recorded in advanced_results')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
