{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ad43fd",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing & Feature Engineering\n",
    "\n",
    "**Objective:** Minimal preprocessing while keeping maximum useful features\n",
    "\n",
    "**Strategy:**\n",
    "1. Filter to Single Family Residential only\n",
    "2. Remove ONLY true leakage (ListPrice, dates, agent/office names, IDs)\n",
    "3. Keep geographic features (City, County, MLSArea, School districts, etc.)\n",
    "4. Handle missing values with higher threshold (50%)\n",
    "5. One-hot encode ALL categories without dropping first (drop_first=False)\n",
    "6. Light feature engineering\n",
    "7. Target outlier removal\n",
    "\n",
    "**Input:** `data/train_raw.csv`, `data/test_raw.csv`  \n",
    "**Output:** `data/X_train.csv`, `data/X_test.csv`, `data/y_train.csv`, `data/y_test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ab480b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:10.515473Z",
     "iopub.status.busy": "2025-10-24T05:04:10.515162Z",
     "iopub.status.idle": "2025-10-24T05:04:10.980586Z",
     "shell.execute_reply": "2025-10-24T05:04:10.979677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run: 2025-10-24T09:47:05.242082\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(f\"Notebook run: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece91549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:10.982921Z",
     "iopub.status.busy": "2025-10-24T05:04:10.982479Z",
     "iopub.status.idle": "2025-10-24T05:04:10.987177Z",
     "shell.execute_reply": "2025-10-24T05:04:10.986355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from:\n",
      "  Train: c:\\Users\\lpnhu\\Downloads\\home-price-prediction\\filled_data\\train_raw.csv\n",
      "  Test: c:\\Users\\lpnhu\\Downloads\\home-price-prediction\\filled_data\\test_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# Paths (works on Windows and Linux/Amarel)\n",
    "from pathlib import Path\n",
    "\n",
    "# Use current working directory as ROOT\n",
    "ROOT = Path.cwd()\n",
    "RAW_DATA_DIR = ROOT / 'filled_data'  # Load from filled_data to get lat/lon columns\n",
    "DATA_DIR = ROOT / 'data'  # For saving processed data\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "\n",
    "train_path = RAW_DATA_DIR / 'train_raw.csv'  # Load from filled_data/\n",
    "test_path = RAW_DATA_DIR / 'test_raw.csv'\n",
    "\n",
    "print(f\"Loading data from:\")\n",
    "print(f\"  Train: {train_path}\")\n",
    "print(f\"  Test: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ebd72e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:10.989041Z",
     "iopub.status.busy": "2025-10-24T05:04:10.988841Z",
     "iopub.status.idle": "2025-10-24T05:04:13.777263Z",
     "shell.execute_reply": "2025-10-24T05:04:13.776234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (151830, 81)\n",
      "Test data: (22972, 81)\n",
      "\n",
      "Note: This is the original raw data with 81 columns\n",
      "We will maximize feature count through lenient preprocessing\n"
     ]
    }
   ],
   "source": [
    "# Load data (already split by month in notebook 01)\n",
    "df_train = pd.read_csv(train_path, low_memory=False)\n",
    "df_test = pd.read_csv(test_path, low_memory=False)\n",
    "\n",
    "print(f\"Training data: {df_train.shape}\")\n",
    "print(f\"Test data: {df_test.shape}\")\n",
    "print(f\"\\nNote: This is the original raw data with {df_train.shape[1]} columns\")\n",
    "print(f\"We will maximize feature count through lenient preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5a706",
   "metadata": {},
   "source": [
    "## Step 1: Filter to Single Family Residential\n",
    "\n",
    "Focus on homogeneous property type for better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bce1390",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:13.779665Z",
     "iopub.status.busy": "2025-10-24T05:04:13.779376Z",
     "iopub.status.idle": "2025-10-24T05:04:13.782868Z",
     "shell.execute_reply": "2025-10-24T05:04:13.782142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping SFR filter - using all property types\n",
      "Train: 151,830, Test: 22,972\n"
     ]
    }
   ],
   "source": [
    "# TESTING: Skip SFR filter to see if all property types perform better\n",
    "print(f\"Skipping SFR filter - using all property types\")\n",
    "print(f\"Train: {len(df_train):,}, Test: {len(df_test):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c359fd",
   "metadata": {},
   "source": [
    "## Step 2: Define Target and Remove Leakage Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fa66be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:13.784528Z",
     "iopub.status.busy": "2025-10-24T05:04:13.784281Z",
     "iopub.status.idle": "2025-10-24T05:04:13.788320Z",
     "shell.execute_reply": "2025-10-24T05:04:13.787662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 37 leakage features to remove\n",
      "Keeping: City, County, PostalCode, PropertyType, YN features, AgentAOR, Schools, Flooring\n"
     ]
    }
   ],
   "source": [
    "# Define ONLY true leakage features - be conservative but thorough!\n",
    "LEAKAGE_FEATURES = [\n",
    "    # Price-related (direct leakage)\n",
    "    'ListPrice', 'OriginalListPrice',\n",
    "    \n",
    "    # ALL Date/time features (anything with 'Date' in name)\n",
    "    'CloseDate', 'DaysOnMarket', 'DOM', 'CDOM',\n",
    "    'ModificationTimestamp', 'StatusChangeTimestamp', 'OnMarketTimestamp',\n",
    "    'ContractDate', 'StatusChangeDate', 'PurchaseContractDate',\n",
    "    'ListingContractDate', 'ContractStatusChangeDate',\n",
    "    \n",
    "    # Agent/Office names (high cardinality, not useful)\n",
    "    'ListAgentEmail', 'ListAgentFirstName', 'ListAgentLastName',\n",
    "    'BuyerAgentEmail', 'BuyerAgentFirstName', 'BuyerAgentLastName',\n",
    "    'CoListAgentFirstName', 'CoListAgentLastName',\n",
    "    'ListOfficeName', 'BuyerOfficeName',\n",
    "    \n",
    "    # Unique IDs\n",
    "    'ListingId', 'ListingKey', 'MLSNumber',\n",
    "    'Matrix_Unique_ID', 'UniversalPropertyId',\n",
    "    \n",
    "    # Address (too unique)\n",
    "    'UnparsedAddress', 'StreetAddress', 'StreetName', 'StreetNumber',\n",
    "    \n",
    "    # Text remarks\n",
    "    'PublicRemarks', 'PrivateRemarks', 'Directions',\n",
    "    \n",
    "    # Source marker\n",
    "    '_source_file'\n",
    "]\n",
    "\n",
    "TARGET = 'ClosePrice'\n",
    "\n",
    "print(f\"Defined {len(LEAKAGE_FEATURES)} leakage features to remove\")\n",
    "print(\"Keeping: City, County, PostalCode, PropertyType, YN features, AgentAOR, Schools, Flooring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fecb275e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:13.790016Z",
     "iopub.status.busy": "2025-10-24T05:04:13.789726Z",
     "iopub.status.idle": "2025-10-24T05:04:13.995977Z",
     "shell.execute_reply": "2025-10-24T05:04:13.995033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 24 leakage columns:\n",
      "  - BuyerAgentFirstName\n",
      "  - BuyerAgentLastName\n",
      "  - BuyerOfficeName\n",
      "  - CloseDate\n",
      "  - CoBuyerAgentFirstName\n",
      "  - CoListAgentFirstName\n",
      "  - CoListAgentLastName\n",
      "  - CoListOfficeName\n",
      "  - ContractStatusChangeDate\n",
      "  - DaysOnMarket\n",
      "  - ListAgentEmail\n",
      "  - ListAgentFirstName\n",
      "  - ListAgentLastName\n",
      "  - ListOfficeName\n",
      "  - ListPrice\n",
      "  ... and 9 more\n",
      "Dropping 24 leakage columns:\n",
      "  - BuyerAgentFirstName\n",
      "  - BuyerAgentLastName\n",
      "  - BuyerOfficeName\n",
      "  - CloseDate\n",
      "  - CoBuyerAgentFirstName\n",
      "  - CoListAgentFirstName\n",
      "  - CoListAgentLastName\n",
      "  - CoListOfficeName\n",
      "  - ContractStatusChangeDate\n",
      "  - DaysOnMarket\n",
      "  - ListAgentEmail\n",
      "  - ListAgentFirstName\n",
      "  - ListAgentLastName\n",
      "  - ListOfficeName\n",
      "  - ListPrice\n",
      "  ... and 9 more\n",
      "\n",
      "Feature matrix shapes after leakage removal:\n",
      "  X_train: (151830, 56)\n",
      "  X_test: (22972, 56)\n",
      "Dropping 24 leakage columns:\n",
      "  - BuyerAgentFirstName\n",
      "  - BuyerAgentLastName\n",
      "  - BuyerOfficeName\n",
      "  - CloseDate\n",
      "  - CoBuyerAgentFirstName\n",
      "  - CoListAgentFirstName\n",
      "  - CoListAgentLastName\n",
      "  - CoListOfficeName\n",
      "  - ContractStatusChangeDate\n",
      "  - DaysOnMarket\n",
      "  - ListAgentEmail\n",
      "  - ListAgentFirstName\n",
      "  - ListAgentLastName\n",
      "  - ListOfficeName\n",
      "  - ListPrice\n",
      "  ... and 9 more\n",
      "\n",
      "Feature matrix shapes after leakage removal:\n",
      "  X_train: (151830, 56)\n",
      "  X_test: (22972, 56)\n"
     ]
    }
   ],
   "source": [
    "def remove_leakage(df, target_col=TARGET):\n",
    "    \"\"\"Remove true leakage features\"\"\"\n",
    "    drop_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        # Check if column matches leakage pattern (case-insensitive)\n",
    "        col_lower = col.lower()\n",
    "        if any(leak.lower() in col_lower for leak in LEAKAGE_FEATURES):\n",
    "            drop_cols.append(col)\n",
    "    \n",
    "    print(f\"Dropping {len(drop_cols)} leakage columns:\")\n",
    "    for col in sorted(drop_cols)[:15]:\n",
    "        print(f\"  - {col}\")\n",
    "    if len(drop_cols) > 15:\n",
    "        print(f\"  ... and {len(drop_cols)-15} more\")\n",
    "    \n",
    "    return df.drop(columns=drop_cols, errors='ignore')\n",
    "\n",
    "# Separate target and features\n",
    "y_train = pd.to_numeric(df_train[TARGET], errors='coerce')\n",
    "y_test = pd.to_numeric(df_test[TARGET], errors='coerce')\n",
    "\n",
    "X_train = remove_leakage(df_train.drop(columns=[TARGET], errors='ignore'))\n",
    "X_test = remove_leakage(df_test.drop(columns=[TARGET], errors='ignore'))\n",
    "\n",
    "print(f\"\\nFeature matrix shapes after leakage removal:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950498ea",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering (Light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15362368",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:13.998637Z",
     "iopub.status.busy": "2025-10-24T05:04:13.998223Z",
     "iopub.status.idle": "2025-10-24T05:04:14.016544Z",
     "shell.execute_reply": "2025-10-24T05:04:14.015718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created BuildingAge\n",
      "Created TotalRooms\n",
      "Created HasGarage\n",
      "\n",
      "Shape after feature engineering: (151830, 59)\n"
     ]
    }
   ],
   "source": [
    "# Create a few useful engineered features\n",
    "current_year = datetime.now().year\n",
    "\n",
    "# BuildingAge\n",
    "if 'YearBuilt' in X_train.columns:\n",
    "    X_train['BuildingAge'] = current_year - pd.to_numeric(X_train['YearBuilt'], errors='coerce')\n",
    "    X_test['BuildingAge'] = current_year - pd.to_numeric(X_test['YearBuilt'], errors='coerce')\n",
    "    X_train['BuildingAge'] = X_train['BuildingAge'].clip(lower=0)\n",
    "    X_test['BuildingAge'] = X_test['BuildingAge'].clip(lower=0)\n",
    "    print(\"Created BuildingAge\")\n",
    "\n",
    "# TotalRooms\n",
    "if 'BedroomsTotal' in X_train.columns and 'BathroomsTotalInteger' in X_train.columns:\n",
    "    X_train['TotalRooms'] = (pd.to_numeric(X_train['BedroomsTotal'], errors='coerce').fillna(0) +\n",
    "                              pd.to_numeric(X_train['BathroomsTotalInteger'], errors='coerce').fillna(0))\n",
    "    X_test['TotalRooms'] = (pd.to_numeric(X_test['BedroomsTotal'], errors='coerce').fillna(0) +\n",
    "                             pd.to_numeric(X_test['BathroomsTotalInteger'], errors='coerce').fillna(0))\n",
    "    print(\"Created TotalRooms\")\n",
    "\n",
    "# HasGarage\n",
    "if 'GarageSpaces' in X_train.columns:\n",
    "    X_train['HasGarage'] = (pd.to_numeric(X_train['GarageSpaces'], errors='coerce').fillna(0) > 0).astype(int)\n",
    "    X_test['HasGarage'] = (pd.to_numeric(X_test['GarageSpaces'], errors='coerce').fillna(0) > 0).astype(int)\n",
    "    print(\"Created HasGarage\")\n",
    "\n",
    "print(f\"\\nShape after feature engineering: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab164064",
   "metadata": {},
   "source": [
    "## Step 4: Handle Missing Values (Lenient Threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7237cc1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:14.018637Z",
     "iopub.status.busy": "2025-10-24T05:04:14.018315Z",
     "iopub.status.idle": "2025-10-24T05:04:14.201860Z",
     "shell.execute_reply": "2025-10-24T05:04:14.200975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 19 columns with >60.0% missing:\n",
      "  - WaterfrontYN: 99.9% missing\n",
      "  - BasementYN: 98.3% missing\n",
      "  - FireplacesTotal: 100.0% missing\n",
      "  - AssociationFeeFrequency: 70.8% missing\n",
      "  - AboveGradeFinishedArea: 100.0% missing\n",
      "  - TaxAnnualAmount: 99.6% missing\n",
      "  - ElementarySchool: 89.1% missing\n",
      "  - BuilderName: 96.2% missing\n",
      "  - SubdivisionName: 64.5% missing\n",
      "  - TaxYear: 99.9% missing\n",
      "  ... and 9 more\n",
      "\n",
      "Shape after dropping high-missing columns: (151830, 40)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with >60% missing (lenient to keep more features)\n",
    "MISSING_THRESHOLD = 0.60\n",
    "\n",
    "missing_pct_train = X_train.isnull().mean()\n",
    "high_missing_cols = missing_pct_train[missing_pct_train > MISSING_THRESHOLD].index.tolist()\n",
    "\n",
    "print(f\"Dropping {len(high_missing_cols)} columns with >{MISSING_THRESHOLD*100}% missing:\")\n",
    "for col in high_missing_cols[:10]:\n",
    "    print(f\"  - {col}: {missing_pct_train[col]*100:.1f}% missing\")\n",
    "if len(high_missing_cols) > 10:\n",
    "    print(f\"  ... and {len(high_missing_cols)-10} more\")\n",
    "\n",
    "X_train = X_train.drop(columns=high_missing_cols)\n",
    "X_test = X_test.drop(columns=high_missing_cols, errors='ignore')\n",
    "\n",
    "print(f\"\\nShape after dropping high-missing columns: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d77ee",
   "metadata": {},
   "source": [
    "## Step 5: Encode Categorical Variables (Keep ALL Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c80c36bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:14.203742Z",
     "iopub.status.busy": "2025-10-24T05:04:14.203435Z",
     "iopub.status.idle": "2025-10-24T05:04:14.208185Z",
     "shell.execute_reply": "2025-10-24T05:04:14.207507Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DEBUG: Before outlier removal ===\n",
      "X_train shape: (151830, 40)\n",
      "X_test shape: (22972, 40)\n",
      "y_train type: <class 'pandas.core.series.Series'>, shape: (151830,)\n",
      "y_test type: <class 'pandas.core.series.Series'>, shape: (22972,)\n",
      "y_train valid count: 151828\n",
      "y_test valid count: 22972\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check shapes before outlier removal\n",
    "print(f\"\\n=== DEBUG: Before outlier removal ===\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train type: {type(y_train)}, shape: {y_train.shape if hasattr(y_train, 'shape') else len(y_train)}\")\n",
    "print(f\"y_test type: {type(y_test)}, shape: {y_test.shape if hasattr(y_test, 'shape') else len(y_test)}\")\n",
    "print(f\"y_train valid count: {y_train.notna().sum() if hasattr(y_train, 'notna') else 'N/A'}\")\n",
    "print(f\"y_test valid count: {y_test.notna().sum() if hasattr(y_test, 'notna') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "893a7135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:14.209903Z",
     "iopub.status.busy": "2025-10-24T05:04:14.209635Z",
     "iopub.status.idle": "2025-10-24T05:04:15.380531Z",
     "shell.execute_reply": "2025-10-24T05:04:15.378795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: 21\n",
      "  One-hot encoding: BuyerAgentAOR (53 unique)\n",
      "  One-hot encoding: ListAgentAOR (53 unique)\n",
      "  One-hot encoding: Flooring (258 unique)\n",
      "  One-hot encoding: ViewYN (2 unique)\n",
      "  One-hot encoding: PoolPrivateYN (2 unique)\n",
      "  One-hot encoding: PropertyType (8 unique)\n",
      "  Target encoding: ListAgentFullName (49994 unique)\n",
      "  Target encoding: BuyerAgentMlsId (65098 unique)\n",
      "  Target encoding: MLSAreaMajor (1046 unique)\n",
      "  One-hot encoding: CountyOrParish (59 unique)\n",
      "  One-hot encoding: MlsStatus (1 unique)\n",
      "  One-hot encoding: AttachedGarageYN (2 unique)\n",
      "  One-hot encoding: PropertySubType (34 unique)\n",
      "  One-hot encoding: BuyerOfficeAOR (60 unique)\n",
      "  Target encoding: City (1009 unique)\n",
      "  One-hot encoding: StateOrProvince (13 unique)\n",
      "  One-hot encoding: FireplaceYN (2 unique)\n",
      "  One-hot encoding: Levels (17 unique)\n",
      "  One-hot encoding: NewConstructionYN (2 unique)\n",
      "  One-hot encoding: HighSchoolDistrict (419 unique)\n",
      "  Target encoding: PostalCode (2127 unique)\n",
      "\n",
      "Target encoding: 5 columns\n",
      "One-hot encoding: 16 columns\n",
      "  Target encoding: ListAgentFullName (49994 unique)\n",
      "  Target encoding: BuyerAgentMlsId (65098 unique)\n",
      "  Target encoding: MLSAreaMajor (1046 unique)\n",
      "  One-hot encoding: CountyOrParish (59 unique)\n",
      "  One-hot encoding: MlsStatus (1 unique)\n",
      "  One-hot encoding: AttachedGarageYN (2 unique)\n",
      "  One-hot encoding: PropertySubType (34 unique)\n",
      "  One-hot encoding: BuyerOfficeAOR (60 unique)\n",
      "  Target encoding: City (1009 unique)\n",
      "  One-hot encoding: StateOrProvince (13 unique)\n",
      "  One-hot encoding: FireplaceYN (2 unique)\n",
      "  One-hot encoding: Levels (17 unique)\n",
      "  One-hot encoding: NewConstructionYN (2 unique)\n",
      "  One-hot encoding: HighSchoolDistrict (419 unique)\n",
      "  Target encoding: PostalCode (2127 unique)\n",
      "\n",
      "Target encoding: 5 columns\n",
      "One-hot encoding: 16 columns\n",
      "\n",
      "One-hot encoding 16 columns...\n",
      "\n",
      "One-hot encoding 16 columns...\n",
      "After encoding: 1022 features!\n",
      "\n",
      "Final feature count: 1022\n",
      "After encoding: 1022 features!\n",
      "\n",
      "Final feature count: 1022\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# Use target encoding for high cardinality (>50 unique values)\n",
    "# This matches Steph's approach and prevents too many sparse one-hot features\n",
    "HIGH_CARD_THRESHOLD = 600\n",
    "target_encode_cols = []\n",
    "onehot_cols = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    n_unique = X_train[col].nunique()\n",
    "    if n_unique > HIGH_CARD_THRESHOLD:\n",
    "        target_encode_cols.append(col)\n",
    "        print(f\"  Target encoding: {col} ({n_unique} unique)\")\n",
    "    else:\n",
    "        onehot_cols.append(col)\n",
    "        print(f\"  One-hot encoding: {col} ({n_unique} unique)\")\n",
    "\n",
    "print(f\"\\nTarget encoding: {len(target_encode_cols)} columns\")\n",
    "print(f\"One-hot encoding: {len(onehot_cols)} columns\")\n",
    "\n",
    "# Target encoding for very high cardinality\n",
    "global_mean = y_train.mean()\n",
    "alpha = 10  # smoothing\n",
    "\n",
    "for col in target_encode_cols:\n",
    "    # Create mapping from training data\n",
    "    stats = X_train[[col]].assign(target=y_train.values).groupby(col).agg(\n",
    "        count=('target', 'size'),\n",
    "        mean=('target', 'mean')\n",
    "    )\n",
    "    stats['smoothed'] = (stats['count'] * stats['mean'] + alpha * global_mean) / (stats['count'] + alpha)\n",
    "    \n",
    "    X_train[f'{col}_target'] = X_train[col].map(stats['smoothed']).fillna(global_mean)\n",
    "    X_test[f'{col}_target'] = X_test[col].map(stats['smoothed']).fillna(global_mean)\n",
    "    \n",
    "    # Drop original categorical column\n",
    "    X_train = X_train.drop(columns=[col])\n",
    "    X_test = X_test.drop(columns=[col])\n",
    "\n",
    "# One-hot encode remaining categoricals - KEEP ALL CATEGORIES (drop_first=False)\n",
    "if onehot_cols:\n",
    "    print(f\"\\nOne-hot encoding {len(onehot_cols)} columns...\")\n",
    "    X_train = pd.get_dummies(X_train, columns=onehot_cols, drop_first=False, dummy_na=False)\n",
    "    X_test = pd.get_dummies(X_test, columns=onehot_cols, drop_first=False, dummy_na=False)\n",
    "    \n",
    "    # Align columns\n",
    "    train_cols = set(X_train.columns)\n",
    "    test_cols = set(X_test.columns)\n",
    "    \n",
    "    for col in train_cols - test_cols:\n",
    "        X_test[col] = 0\n",
    "    for col in test_cols - train_cols:\n",
    "        X_train[col] = 0\n",
    "    \n",
    "    X_test = X_test[X_train.columns]\n",
    "    \n",
    "    print(f\"After encoding: {X_train.shape[1]} features!\")\n",
    "\n",
    "print(f\"\\nFinal feature count: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b151a7a",
   "metadata": {},
   "source": [
    "## Step 6: Remove Target Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d8169ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:15.383291Z",
     "iopub.status.busy": "2025-10-24T05:04:15.382996Z",
     "iopub.status.idle": "2025-10-24T05:04:16.346940Z",
     "shell.execute_reply": "2025-10-24T05:04:16.346042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before outlier removal: X_train=(151830, 1022), y_train=151830\n",
      "Outlier bounds: $1,500 to $6,807,595\n",
      "Keeping 150,311 of 151,830 training samples\n",
      "\n",
      "Test outlier bounds: $1,500 to $6,750,000\n",
      "Keeping 22,759 of 22,972 test samples\n",
      "\n",
      "After outlier removal:\n",
      "  X_train: (150311, 1022)\n",
      "  X_test: (22759, 1022)\n",
      "\n",
      "Test outlier bounds: $1,500 to $6,750,000\n",
      "Keeping 22,759 of 22,972 test samples\n",
      "\n",
      "After outlier removal:\n",
      "  X_train: (150311, 1022)\n",
      "  X_test: (22759, 1022)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reset indices to ensure alignment\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "print(f\"Before outlier removal: X_train={X_train.shape}, y_train={len(y_train)}\")\n",
    "\n",
    "# Remove extreme outliers from training set\n",
    "y_valid_train = y_train.dropna()\n",
    "p_low = np.percentile(y_valid_train, 0.5)\n",
    "p_high = np.percentile(y_valid_train, 99.5)\n",
    "keep_mask = (y_train >= p_low) & (y_train <= p_high) & y_train.notna()\n",
    "\n",
    "print(f\"Outlier bounds: ${p_low:,.0f} to ${p_high:,.0f}\")\n",
    "print(f\"Keeping {keep_mask.sum():,} of {len(y_train):,} training samples\")\n",
    "\n",
    "X_train = X_train[keep_mask].reset_index(drop=True)\n",
    "y_train = y_train[keep_mask].reset_index(drop=True)\n",
    "\n",
    "# Remove outliers from test set\n",
    "y_valid_test = y_test.dropna()\n",
    "p_low_test = np.percentile(y_valid_test, 0.5)\n",
    "p_high_test = np.percentile(y_valid_test, 99.5)\n",
    "keep_mask_test = (y_test >= p_low_test) & (y_test <= p_high_test) & y_test.notna()\n",
    "\n",
    "print(f\"\\nTest outlier bounds: ${p_low_test:,.0f} to ${p_high_test:,.0f}\")\n",
    "print(f\"Keeping {keep_mask_test.sum():,} of {len(y_test):,} test samples\")\n",
    "\n",
    "X_test = X_test[keep_mask_test].reset_index(drop=True)\n",
    "y_test = y_test[keep_mask_test].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nAfter outlier removal:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acf948",
   "metadata": {},
   "source": [
    "## Step 7: Final Imputation\n",
    "\n",
    "Fill any remaining missing values with median (numeric columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9dbc0ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:16.348915Z",
     "iopub.status.busy": "2025-10-24T05:04:16.348590Z",
     "iopub.status.idle": "2025-10-24T05:04:16.675656Z",
     "shell.execute_reply": "2025-10-24T05:04:16.674784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      "  X_train: 0\n",
      "  X_test: 0\n",
      "  X_train: 0\n",
      "  X_test: 0\n"
     ]
    }
   ],
   "source": [
    "# Impute remaining missing values with median for numeric columns\n",
    "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    median_val = X_train[col].median()\n",
    "    X_train[col].fillna(median_val, inplace=True)\n",
    "    X_test[col].fillna(median_val, inplace=True)\n",
    "\n",
    "print(f\"Missing values after imputation:\")\n",
    "print(f\"  X_train: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"  X_test: {X_test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05c740",
   "metadata": {},
   "source": [
    "## Step 8: Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "183c455a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-24T05:04:16.677839Z",
     "iopub.status.busy": "2025-10-24T05:04:16.677516Z",
     "iopub.status.idle": "2025-10-24T05:05:11.019351Z",
     "shell.execute_reply": "2025-10-24T05:05:11.018166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed data saved!\n",
      "Final feature count: 1022\n",
      "Training samples: 150311\n",
      "Test samples: 22759\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save processed data\n",
    "X_train.to_csv(ROOT / 'filled_data' / 'X_train.csv', index=False)\n",
    "X_test.to_csv(ROOT / 'filled_data' / 'X_test.csv', index=False)\n",
    "y_train.to_csv(ROOT / 'filled_data' / 'y_train.csv', index=False)\n",
    "y_test.to_csv(ROOT / 'filled_data' / 'y_test.csv', index=False)\n",
    "\n",
    "# Also save to models folder for pipeline compatibility\n",
    "X_train.to_csv(ROOT / 'models' / 'X_train.csv', index=False)\n",
    "X_test.to_csv(ROOT / 'models' / 'X_test.csv', index=False)\n",
    "y_train.to_csv(ROOT / 'models' / 'y_train.csv', index=False)\n",
    "y_test.to_csv(ROOT / 'models' / 'y_test.csv', index=False)\n",
    "\n",
    "# Save feature schema\n",
    "feature_schema = {\n",
    "    'feature_columns': list(X_train.columns),\n",
    "    'n_features': len(X_train.columns),\n",
    "    'n_train_samples': len(X_train),\n",
    "    'n_test_samples': len(X_test)\n",
    "}\n",
    "with open(ROOT / 'models' / 'feature_schema.json', 'w') as f:\n",
    "    json.dump(feature_schema, f, indent=2)\n",
    "\n",
    "print(f\"\\nPreprocessed data saved!\")\n",
    "print(f\"Final feature count: {len(X_train.columns)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
