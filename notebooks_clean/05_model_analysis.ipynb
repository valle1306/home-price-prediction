{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42733ffa",
   "metadata": {},
   "source": [
    "# 05 - Model Analysis & Visualization\n",
    "\n",
    "**Objective:** Understand what drives our model's predictions\n",
    "\n",
    "**Analysis:**\n",
    "1. Feature Importance (Random Forest, XGBoost, LightGBM)\n",
    "2. SHAP Values (explain individual predictions)\n",
    "3. Prediction Error Analysis\n",
    "4. Feature Correlations\n",
    "5. Geographic Patterns (if lat/lon important)\n",
    "\n",
    "**Input:** Trained models from notebook 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "import shap\n",
    "\n",
    "# Plotting settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"Model Analysis & Visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea99abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and best model\n",
    "ROOT = Path(r\"c:\\Users\\lpnhu\\Downloads\\home-price-prediction\")\n",
    "DATA_DIR = ROOT / 'data'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "PLOTS_DIR = ROOT / 'plots'\n",
    "PLOTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "X_train = pd.read_csv(DATA_DIR / 'X_train.csv')\n",
    "X_test = pd.read_csv(DATA_DIR / 'X_test.csv')\n",
    "y_train = pd.read_csv(DATA_DIR / 'y_train.csv')['ClosePrice'].values\n",
    "y_test = pd.read_csv(DATA_DIR / 'y_test.csv')['ClosePrice'].values\n",
    "\n",
    "# Load best model\n",
    "best_model = joblib.load(MODELS_DIR / 'best_advanced_model.joblib')\n",
    "\n",
    "# Load summary\n",
    "with open(MODELS_DIR / 'advanced_models_summary.json') as f:\n",
    "    summary = json.load(f)\n",
    "\n",
    "print(f\"Best Model: {summary['best_model']}\")\n",
    "print(f\"Test R²: {summary['best_r2']:.4f}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Samples: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e266f",
   "metadata": {},
   "source": [
    "## 1. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3032dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Top 30 features\n",
    "    top_30 = importance_df.head(30)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(range(len(top_30)), top_30['importance'], color='steelblue')\n",
    "    plt.yticks(range(len(top_30)), top_30['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 30 Features - {summary[\"best_model\"]}')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'feature_importance_top30.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 20 Most Important Features:\")\n",
    "    print(importance_df.head(20).to_string(index=False))\n",
    "    \n",
    "    # Save full importance\n",
    "    importance_df.to_csv(MODELS_DIR / 'feature_importance.csv', index=False)\n",
    "else:\n",
    "    print(\"Model doesn't support feature_importances_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73b3d9e",
   "metadata": {},
   "source": [
    "## 2. Prediction vs Actual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f75578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, y_pred_test, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', lw=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Price ($)')\n",
    "axes[0].set_ylabel('Predicted Price ($)')\n",
    "axes[0].set_title(f'Actual vs Predicted (R² = {summary[\"best_r2\"]:.4f})')\n",
    "axes[0].legend()\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - y_pred_test\n",
    "axes[1].scatter(y_pred_test, residuals, alpha=0.3, s=10)\n",
    "axes[1].axhline(0, color='r', linestyle='--', lw=2)\n",
    "axes[1].set_xlabel('Predicted Price ($)')\n",
    "axes[1].set_ylabel('Residuals ($)')\n",
    "axes[1].set_title('Residual Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Residual statistics\n",
    "print(\"\\nResidual Statistics:\")\n",
    "print(f\"Mean Residual: ${residuals.mean():,.0f}\")\n",
    "print(f\"Std Residual: ${residuals.std():,.0f}\")\n",
    "print(f\"MAE: ${mean_absolute_error(y_test, y_pred_test):,.0f}\")\n",
    "print(f\"Median Residual: ${np.median(residuals):,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d869eb",
   "metadata": {},
   "source": [
    "## 3. Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0848ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage errors\n",
    "pct_errors = (residuals / y_test) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute residuals histogram\n",
    "axes[0].hist(residuals / 1000, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Residual ($1000s)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Residuals')\n",
    "axes[0].axvline(0, color='r', linestyle='--', lw=2)\n",
    "\n",
    "# Percentage errors\n",
    "axes[1].hist(pct_errors, bins=50, edgecolor='black', alpha=0.7, range=(-50, 50))\n",
    "axes[1].set_xlabel('Percentage Error (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Percentage Errors')\n",
    "axes[1].axvline(0, color='r', linestyle='--', lw=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'error_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPercentage Error Statistics:\")\n",
    "print(f\"Mean: {pct_errors.mean():.2f}%\")\n",
    "print(f\"Median: {np.median(pct_errors):.2f}%\")\n",
    "print(f\"Std: {pct_errors.std():.2f}%\")\n",
    "print(f\"Within ±10%: {(np.abs(pct_errors) <= 10).sum() / len(pct_errors) * 100:.1f}%\")\n",
    "print(f\"Within ±20%: {(np.abs(pct_errors) <= 20).sum() / len(pct_errors) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2386f7ae",
   "metadata": {},
   "source": [
    "## 4. Price Range Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354dc95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bin prices into ranges\n",
    "price_bins = [0, 200000, 400000, 600000, 800000, 1000000, np.inf]\n",
    "labels = ['<200K', '200-400K', '400-600K', '600-800K', '800K-1M', '>1M']\n",
    "\n",
    "df_analysis = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred_test,\n",
    "    'residual': residuals,\n",
    "    'pct_error': pct_errors,\n",
    "    'price_range': pd.cut(y_test, bins=price_bins, labels=labels)\n",
    "})\n",
    "\n",
    "# Performance by price range\n",
    "range_stats = df_analysis.groupby('price_range').agg({\n",
    "    'actual': 'count',\n",
    "    'pct_error': ['mean', 'std'],\n",
    "    'residual': lambda x: r2_score(df_analysis.loc[x.index, 'actual'], \n",
    "                                   df_analysis.loc[x.index, 'predicted'])\n",
    "}).round(2)\n",
    "\n",
    "range_stats.columns = ['Count', 'Mean % Error', 'Std % Error', 'R²']\n",
    "print(\"\\nPerformance by Price Range:\")\n",
    "print(range_stats.to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Mean absolute % error by range\n",
    "range_stats['Mean Abs % Error'] = df_analysis.groupby('price_range')['pct_error'].apply(lambda x: np.abs(x).mean())\n",
    "range_stats['Mean Abs % Error'].plot(kind='bar', ax=axes[0], color='steelblue')\n",
    "axes[0].set_xlabel('Price Range')\n",
    "axes[0].set_ylabel('Mean Absolute % Error')\n",
    "axes[0].set_title('Error by Price Range')\n",
    "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45)\n",
    "\n",
    "# R² by range\n",
    "range_stats['R²'].plot(kind='bar', ax=axes[1], color='green')\n",
    "axes[1].set_xlabel('Price Range')\n",
    "axes[1].set_ylabel('R²')\n",
    "axes[1].set_title('R² by Price Range')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45)\n",
    "axes[1].axhline(summary['best_r2'], color='r', linestyle='--', label='Overall R²')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PLOTS_DIR / 'performance_by_price_range.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ed79f",
   "metadata": {},
   "source": [
    "## 5. SHAP Values (Feature Impact on Predictions)\n",
    "\n",
    "**Note:** This can be slow for large datasets. We'll use a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c5c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating SHAP values (this may take a few minutes)...\\n\")\n",
    "\n",
    "# Sample for SHAP (1000 samples for speed)\n",
    "sample_size = min(1000, len(X_test))\n",
    "sample_idx = np.random.choice(len(X_test), sample_size, replace=False)\n",
    "X_sample = X_test.iloc[sample_idx]\n",
    "\n",
    "try:\n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    # Summary plot\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values, X_sample, max_display=20, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance (mean abs SHAP)\n",
    "    shap_importance = pd.DataFrame({\n",
    "        'feature': X_sample.columns,\n",
    "        'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n",
    "    }).sort_values('mean_abs_shap', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 20 Features by Mean |SHAP|:\")\n",
    "    print(shap_importance.head(20).to_string(index=False))\n",
    "    \n",
    "    shap_importance.to_csv(MODELS_DIR / 'shap_importance.csv', index=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"SHAP calculation failed: {e}\")\n",
    "    print(\"Skipping SHAP analysis (may not be supported for this model type)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2717a986",
   "metadata": {},
   "source": [
    "## 6. Top Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09929aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 15 features\n",
    "if 'importance_df' in locals():\n",
    "    top_features = importance_df.head(15)['feature'].tolist()\n",
    "    \n",
    "    # Correlation matrix\n",
    "    corr_matrix = X_train[top_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1)\n",
    "    plt.title('Correlation Matrix - Top 15 Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOTS_DIR / 'feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d9bee",
   "metadata": {},
   "source": [
    "## 7. Key Insights Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70150f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. Model Performance:\")\n",
    "print(f\"   - Best Model: {summary['best_model']}\")\n",
    "print(f\"   - Test R²: {summary['best_r2']:.4f}\")\n",
    "print(f\"   - vs Steph: {summary['improvement']:+.4f}\")\n",
    "\n",
    "if 'importance_df' in locals():\n",
    "    print(f\"\\n2. Top 5 Most Important Features:\")\n",
    "    for i, row in importance_df.head(5).iterrows():\n",
    "        print(f\"   {i+1}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. Prediction Accuracy:\")\n",
    "print(f\"   - Mean Absolute Error: ${mean_absolute_error(y_test, y_pred_test):,.0f}\")\n",
    "print(f\"   - Predictions within ±10%: {(np.abs(pct_errors) <= 10).sum() / len(pct_errors) * 100:.1f}%\")\n",
    "print(f\"   - Predictions within ±20%: {(np.abs(pct_errors) <= 20).sum() / len(pct_errors) * 100:.1f}%\")\n",
    "\n",
    "print(f\"\\n4. Model Characteristics:\")\n",
    "print(f\"   - Systematic bias: {residuals.mean():+,.0f} (near 0 is good)\")\n",
    "print(f\"   - Residual spread: ${residuals.std():,.0f}\")\n",
    "\n",
    "print(f\"\\n5. Recommendations:\")\n",
    "if summary['best_r2'] > 0.884:\n",
    "    print(\"   ✅ Model beats Steph's baseline!\")\n",
    "    print(\"   - Consider ensemble methods for further improvement\")\n",
    "    print(\"   - Analyze feature engineering opportunities\")\n",
    "else:\n",
    "    gap = (0.884 - summary['best_r2']) * 100\n",
    "    print(f\"   ⚠️  Still {gap:.2f}% behind Steph\")\n",
    "    print(\"   - Try stacking/blending multiple models\")\n",
    "    print(\"   - Consider more aggressive feature engineering\")\n",
    "    print(\"   - Investigate outliers and data quality\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Plots saved to: {PLOTS_DIR}\")\n",
    "print(f\"Analysis files saved to: {MODELS_DIR}\")\n",
    "print(\"\\n✅ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
