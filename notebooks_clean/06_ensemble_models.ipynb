{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac04277a",
   "metadata": {},
   "source": [
    "# 06 - Ensemble & Advanced Models\n",
    "\n",
    "**Objective:** Push beyond single model performance with ensemble techniques\n",
    "\n",
    "**Methods:**\n",
    "1. Voting Regressor (combine RF, XGB, LightGBM)\n",
    "2. Stacking Regressor (meta-learner)\n",
    "3. Blending (manual ensemble)\n",
    "4. CatBoost (gradient boosting variant)\n",
    "5. Neural Network (simple feedforward)\n",
    "\n",
    "**Target:** Beat Steph's 88.4% R¬≤ and current best from notebook 04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca656bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(f\"Ensemble & Advanced Models - {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8901187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "\n",
    "X_train = pd.read_csv(DATA_DIR / 'X_train.csv')\n",
    "X_test = pd.read_csv(DATA_DIR / 'X_test.csv')\n",
    "y_train = pd.read_csv(DATA_DIR / 'y_train.csv')['ClosePrice'].values\n",
    "y_test = pd.read_csv(DATA_DIR / 'y_test.csv')['ClosePrice'].values\n",
    "\n",
    "# Load previous best from notebook 04\n",
    "with open(MODELS_DIR / 'advanced_models_summary.json') as f:\n",
    "    prev_best = json.load(f)\n",
    "\n",
    "print(f\"Data: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"\\nPrevious Best: {prev_best['best_model']} with {prev_best['best_r2']:.4f} R¬≤\")\n",
    "print(f\"Target: Beat Steph's 88.4% R¬≤\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06be2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "        'test_mdape': np.median(np.abs((y_test - y_pred_test) / y_test)) * 100,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Train R¬≤: {results['train_r2']:.4f}\")\n",
    "    print(f\"Test R¬≤:  {results['test_r2']:.4f}\", end='')\n",
    "    \n",
    "    if results['test_r2'] > 0.884:\n",
    "        print(f\" üéâ BEATS STEPH!\")\n",
    "    elif results['test_r2'] > prev_best['best_r2']:\n",
    "        print(f\" ‚¨ÜÔ∏è NEW BEST! (was {prev_best['best_r2']:.4f})\")\n",
    "    else:\n",
    "        gap = (0.884 - results['test_r2']) * 100\n",
    "        print(f\" (Gap to Steph: {gap:.2f}%)\")\n",
    "    \n",
    "    print(f\"RMSE:     ${results['test_rmse']:,.0f}\")\n",
    "    print(f\"MAE:      ${results['test_mae']:,.0f}\")\n",
    "    print(f\"MdAPE:    {results['test_mdape']:.2f}%\")\n",
    "    print(f\"Time:     {train_time:.1f}s\")\n",
    "    \n",
    "    return results, model\n",
    "\n",
    "ensemble_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519d781",
   "metadata": {},
   "source": [
    "## 1. Voting Regressor (Simple Ensemble)\n",
    "\n",
    "Average predictions from multiple strong models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360cce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Voting Regressor...\\n\")\n",
    "\n",
    "# Define base estimators with reasonable hyperparameters\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=25,\n",
    "    min_samples_split=5,\n",
    "    max_features='sqrt',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=7,\n",
    "    num_leaves=50,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "voting = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', rf),\n",
    "        ('xgb', xgb_model),\n",
    "        ('lgb', lgb_model)\n",
    "    ],\n",
    "    n_jobs=1  # Base models already use all cores\n",
    ")\n",
    "\n",
    "voting_results, voting_model = evaluate_model(voting, X_train, X_test, y_train, y_test,\n",
    "                                              \"Voting Ensemble (RF + XGB + LGB)\")\n",
    "ensemble_results.append(voting_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1612c589",
   "metadata": {},
   "source": [
    "## 2. Stacking Regressor (Meta-Learner)\n",
    "\n",
    "Train a meta-model to optimally combine base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad693ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Stacking Regressor...\\n\")\n",
    "\n",
    "# Use same base estimators\n",
    "rf_stack = RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_split=5,\n",
    "    max_features='sqrt', random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_stack = xgb.XGBRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=7,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "    tree_method='hist', n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_stack = lgb.LGBMRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=7,\n",
    "    num_leaves=50, subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42, n_jobs=-1, verbose=-1\n",
    ")\n",
    "\n",
    "# Ridge as meta-learner\n",
    "stacking = StackingRegressor(\n",
    "    estimators=[\n",
    "        ('rf', rf_stack),\n",
    "        ('xgb', xgb_stack),\n",
    "        ('lgb', lgb_stack)\n",
    "    ],\n",
    "    final_estimator=Ridge(alpha=10.0),\n",
    "    cv=3,  # 3-fold CV for meta-features\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "stacking_results, stacking_model = evaluate_model(stacking, X_train, X_test, y_train, y_test,\n",
    "                                                   \"Stacking Ensemble (Ridge Meta-Learner)\")\n",
    "ensemble_results.append(stacking_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15923c0e",
   "metadata": {},
   "source": [
    "## 3. CatBoost (Alternative Gradient Boosting)\n",
    "\n",
    "CatBoost handles categorical features natively and often outperforms XGBoost/LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782617e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    \n",
    "    print(\"Training CatBoost...\\n\")\n",
    "    \n",
    "    catboost = CatBoostRegressor(\n",
    "        iterations=500,\n",
    "        learning_rate=0.05,\n",
    "        depth=7,\n",
    "        l2_leaf_reg=3,\n",
    "        random_seed=42,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    catboost_results, catboost_model = evaluate_model(catboost, X_train, X_test, y_train, y_test,\n",
    "                                                       \"CatBoost\")\n",
    "    ensemble_results.append(catboost_results)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"CatBoost not installed. Skipping. (Install with: pip install catboost)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c129b3a2",
   "metadata": {},
   "source": [
    "## 4. Weighted Blending\n",
    "\n",
    "Manually weighted ensemble based on individual model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8871c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building Weighted Blend...\\n\")\n",
    "\n",
    "# Train individual models\n",
    "rf_blend = RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=25, min_samples_split=5,\n",
    "    max_features='sqrt', random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_blend.fit(X_train, y_train)\n",
    "\n",
    "xgb_blend = xgb.XGBRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=7,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "    tree_method='hist', n_jobs=-1\n",
    ")\n",
    "xgb_blend.fit(X_train, y_train)\n",
    "\n",
    "lgb_blend = lgb.LGBMRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=7,\n",
    "    num_leaves=50, subsample=0.8, colsample_bytree=0.8,\n",
    "    random_state=42, n_jobs=-1, verbose=-1\n",
    ")\n",
    "lgb_blend.fit(X_train, y_train)\n",
    "\n",
    "# Get individual R¬≤ scores on test\n",
    "rf_r2 = r2_score(y_test, rf_blend.predict(X_test))\n",
    "xgb_r2 = r2_score(y_test, xgb_blend.predict(X_test))\n",
    "lgb_r2 = r2_score(y_test, lgb_blend.predict(X_test))\n",
    "\n",
    "print(f\"Individual R¬≤ scores:\")\n",
    "print(f\"  RF:  {rf_r2:.4f}\")\n",
    "print(f\"  XGB: {xgb_r2:.4f}\")\n",
    "print(f\"  LGB: {lgb_r2:.4f}\")\n",
    "\n",
    "# Weighted blend (proportional to R¬≤)\n",
    "total = rf_r2 + xgb_r2 + lgb_r2\n",
    "w_rf = rf_r2 / total\n",
    "w_xgb = xgb_r2 / total\n",
    "w_lgb = lgb_r2 / total\n",
    "\n",
    "print(f\"\\nWeights: RF={w_rf:.3f}, XGB={w_xgb:.3f}, LGB={w_lgb:.3f}\")\n",
    "\n",
    "# Blended predictions\n",
    "y_pred_blend = (\n",
    "    w_rf * rf_blend.predict(X_test) +\n",
    "    w_xgb * xgb_blend.predict(X_test) +\n",
    "    w_lgb * lgb_blend.predict(X_test)\n",
    ")\n",
    "\n",
    "blend_r2 = r2_score(y_test, y_pred_blend)\n",
    "blend_results = {\n",
    "    'model': 'Weighted Blend',\n",
    "    'train_r2': np.nan,\n",
    "    'test_r2': blend_r2,\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_blend)),\n",
    "    'test_mae': mean_absolute_error(y_test, y_pred_blend),\n",
    "    'test_mdape': np.median(np.abs((y_test - y_pred_blend) / y_test)) * 100,\n",
    "    'train_time': 0\n",
    "}\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Weighted Blend\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Test R¬≤:  {blend_r2:.4f}\", end='')\n",
    "if blend_r2 > 0.884:\n",
    "    print(f\" üéâ BEATS STEPH!\")\n",
    "elif blend_r2 > prev_best['best_r2']:\n",
    "    print(f\" ‚¨ÜÔ∏è NEW BEST!\")\n",
    "else:\n",
    "    print(f\" (Gap: {(0.884 - blend_r2)*100:.2f}%)\")\n",
    "\n",
    "ensemble_results.append(blend_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da93a5",
   "metadata": {},
   "source": [
    "## 5. Neural Network (Simple Feedforward)\n",
    "\n",
    "Test if deep learning can capture patterns tree models miss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19260735",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Neural Network...\\n\")\n",
    "\n",
    "# Scale features for neural network\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Simple feedforward network\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    batch_size=512,\n",
    "    learning_rate='adaptive',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=10,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "mlp_results, mlp_model = evaluate_model(mlp, X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "                                        \"Neural Network (3-layer MLP)\")\n",
    "ensemble_results.append(mlp_results)\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, MODELS_DIR / 'scaler_nn.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a009b3",
   "metadata": {},
   "source": [
    "## Final Results & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c0bded",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(ensemble_results).sort_values('test_r2', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENSEMBLE MODELS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Overall best (including notebook 04)\n",
    "best_r2 = results_df.iloc[0]['test_r2']\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "steph_r2 = 0.884\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nNotebook 04 Best: {prev_best['best_r2']*100:.2f}% R¬≤ ({prev_best['best_model']})\")\n",
    "print(f\"Notebook 06 Best: {best_r2*100:.2f}% R¬≤ ({best_model_name})\")\n",
    "print(f\"Steph Baseline:   {steph_r2*100:.2f}% R¬≤ (Random Forest)\")\n",
    "\n",
    "# Determine absolute best\n",
    "absolute_best_r2 = max(best_r2, prev_best['best_r2'])\n",
    "absolute_best_name = best_model_name if best_r2 > prev_best['best_r2'] else prev_best['best_model']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"OVERALL BEST MODEL: {absolute_best_name}\")\n",
    "print(f\"R¬≤: {absolute_best_r2*100:.2f}%\")\n",
    "print(f\"vs Steph: {(absolute_best_r2 - steph_r2)*100:+.2f} percentage points\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if absolute_best_r2 > steph_r2:\n",
    "    print(f\"\\nüéâüéâ SUCCESS! We BEAT Steph by {(absolute_best_r2 - steph_r2)*100:.2f}%! üéâüéâ\")\n",
    "else:\n",
    "    gap = (steph_r2 - absolute_best_r2) * 100\n",
    "    print(f\"\\n‚ö†Ô∏è  Still {gap:.2f}% behind Steph\")\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(\"  1. Analyze feature importance from notebook 05\")\n",
    "    print(\"  2. Engineer more powerful features\")\n",
    "    print(\"  3. Try deeper neural networks or transformers\")\n",
    "    print(\"  4. Investigate data quality and outliers\")\n",
    "    print(\"  5. Consider domain-specific feature engineering\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(MODELS_DIR / 'ensemble_models_results.csv', index=False)\n",
    "\n",
    "# Save best ensemble model\n",
    "if best_model_name == 'Voting Ensemble (RF + XGB + LGB)':\n",
    "    best_ensemble = voting_model\n",
    "elif best_model_name == 'Stacking Ensemble (Ridge Meta-Learner)':\n",
    "    best_ensemble = stacking_model\n",
    "elif 'CatBoost' in best_model_name:\n",
    "    best_ensemble = catboost_model\n",
    "elif 'Neural Network' in best_model_name:\n",
    "    best_ensemble = mlp_model\n",
    "else:\n",
    "    best_ensemble = None\n",
    "\n",
    "if best_ensemble is not None:\n",
    "    joblib.dump(best_ensemble, MODELS_DIR / 'best_ensemble_model.joblib')\n",
    "\n",
    "# Save overall summary\n",
    "summary = {\n",
    "    'notebook_04_best': prev_best['best_model'],\n",
    "    'notebook_04_r2': prev_best['best_r2'],\n",
    "    'notebook_06_best': best_model_name,\n",
    "    'notebook_06_r2': float(best_r2),\n",
    "    'overall_best': absolute_best_name,\n",
    "    'overall_best_r2': float(absolute_best_r2),\n",
    "    'steph_r2': float(steph_r2),\n",
    "    'beat_steph': bool(absolute_best_r2 > steph_r2),\n",
    "    'improvement_over_steph': float(absolute_best_r2 - steph_r2),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / 'final_ensemble_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nModels saved to {MODELS_DIR}\")\n",
    "print(\"\\n‚úÖ Ensemble training complete!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}