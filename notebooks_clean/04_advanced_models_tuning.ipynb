{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacc2816",
   "metadata": {},
   "source": [
    "# 04 - Advanced Models & Hyperparameter Tuning\n",
    "\n",
    "**Objective:** Train and tune advanced tree-based models to beat Steph's 88.4% R²\n",
    "\n",
    "**Models:**\n",
    "1. Random Forest (Tuned)\n",
    "2. Gradient Boosting (Tuned)\n",
    "3. XGBoost (Tuned)\n",
    "4. LightGBM (Tuned)\n",
    "\n",
    "**Current Best:** 83.91% R² (XGBoost basic)\n",
    "\n",
    "**Input:** `data/X_train.csv`, `data/X_test.csv`, `data/y_train.csv`, `data/y_test.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aff6d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run: 2025-10-24T09:55:24.208397\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "import joblib\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "print(f\"Notebook run: {datetime.now().isoformat()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 150,311 samples, 1022 features\n",
      "Testing: 22,759 samples\n",
      "\n",
      "Target: Beat Steph's 88.4% R²\n"
     ]
    }
   ],
   "source": [
    "# Load processed data (portable path)\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / 'data'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "X_train = pd.read_csv(DATA_DIR / 'X_train.csv')\n",
    "X_test = pd.read_csv(DATA_DIR / 'X_test.csv')\n",
    "y_train = pd.read_csv(DATA_DIR / 'y_train.csv')['ClosePrice'].values\n",
    "y_test = pd.read_csv(DATA_DIR / 'y_test.csv')['ClosePrice'].values\n",
    "\n",
    "# Sanitize feature names: LightGBM doesn't accept special JSON characters in column names\n",
    "import re\n",
    "def _sanitize_columns(cols):\n",
    "    return [re.sub(r'[^0-9A-Za-z_]', '_', str(c)) for c in cols]\n",
    "X_train.columns = _sanitize_columns(X_train.columns)\n",
    "X_test.columns = _sanitize_columns(X_test.columns)\n",
    "\n",
    "print(f\"Training: {X_train.shape[0]:,} samples, {X_train.shape[1]} features\")\n",
    "print(f\"Testing: {X_test.shape[0]:,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test),\n",
    "        'test_mdape': np.median(np.abs((y_test - y_pred_test) / y_test)) * 100,\n",
    "        'train_time': train_time\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Train R²: {results['train_r2']:.4f}\")\n",
    "    # Build a status string to avoid nested f-strings and quoting issues\n",
    "    steph_threshold = 0.884\n",
    "    if results['test_r2'] > steph_threshold:\n",
    "        status = '🎉 BEATS STEPH!'\n",
    "    else:\n",
    "        status = f\"(Gap: {(steph_threshold - results['test_r2']) * 100:.2f}% to Steph)\"\n",
    "    print(f\"Test R²:  {results['test_r2']:.4f} {status}\")\n",
    "    print(f\"RMSE:     ${results['test_rmse']:,.0f}\")\n",
    "    print(f\"MAE:      ${results['test_mae']:,.0f}\")\n",
    "    print(f\"MdAPE:    {results['test_mdape']:.2f}%\")\n",
    "    print(f\"Time:     {train_time:.1f}s\")\n",
    "\n",
    "    return results, model\n",
    "\n",
    "advanced_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b3ae5a",
   "metadata": {},
   "source": [
    "## 1. Random Forest (Hyperparameter Tuning)\n",
    "\n",
    "Steph's best model was Random Forest with 88.4% R². Let's tune it aggressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d9e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Random Forest (this will take several minutes)...\n",
      "\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "print(\"Tuning Random Forest (this will take several minutes)...\\n\")\n",
    "\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [15, 20, 25, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.3, 0.5],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_distributions=rf_param_dist,\n",
    "    n_iter=20,  # Test 20 different combinations\n",
    "    cv=3,  # 3-fold CV to save memory\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=4  # Limit parallel jobs\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest Random Forest params: {rf_search.best_params_}\")\n",
    "print(f\"Best CV R²: {rf_search.best_score_:.4f}\")\n",
    "\n",
    "rf_results, rf_model = evaluate_model(rf_search.best_estimator_, X_train, X_test, \n",
    "                                      y_train, y_test, \"Random Forest (Tuned)\")\n",
    "advanced_results.append(rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3eabaf",
   "metadata": {},
   "source": [
    "## 2. Gradient Boosting (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning Gradient Boosting...\\n\")\n",
    "\n",
    "gb_param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'max_features': ['sqrt', 'log2', 0.3, 0.5]\n",
    "}\n",
    "\n",
    "gb_search = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_distributions=gb_param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "gb_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest Gradient Boosting params: {gb_search.best_params_}\")\n",
    "print(f\"Best CV R²: {gb_search.best_score_:.4f}\")\n",
    "\n",
    "gb_results, gb_model = evaluate_model(gb_search.best_estimator_, X_train, X_test,\n",
    "                                      y_train, y_test, \"Gradient Boosting (Tuned)\")\n",
    "advanced_results.append(gb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bbed4a",
   "metadata": {},
   "source": [
    "## 3. XGBoost (Aggressive Hyperparameter Tuning)\n",
    "\n",
    "Current best: 83.91% R². Let's push it higher!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34255b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning XGBoost (current best: 83.91%)...\\n\")\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [5, 7, 9, 11],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [1, 1.5, 2]\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb.XGBRegressor(random_state=42, tree_method='hist', n_jobs=-1),\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=25,  # More iterations for better tuning\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest XGBoost params: {xgb_search.best_params_}\")\n",
    "print(f\"Best CV R²: {xgb_search.best_score_:.4f}\")\n",
    "\n",
    "xgb_results, xgb_model = evaluate_model(xgb_search.best_estimator_, X_train, X_test,\n",
    "                                        y_train, y_test, \"XGBoost (Tuned)\")\n",
    "advanced_results.append(xgb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02da6afa",
   "metadata": {},
   "source": [
    "## 4. LightGBM (Fast Gradient Boosting)\n",
    "\n",
    "LightGBM is faster and often more accurate than XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf6f7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tuning LightGBM...\\n\")\n",
    "\n",
    "lgb_param_dist = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [5, 7, 9, -1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'min_child_samples': [20, 30, 50],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    lgb.LGBMRegressor(random_state=42, n_jobs=-1, verbose=-1),\n",
    "    param_distributions=lgb_param_dist,\n",
    "    n_iter=25,\n",
    "    cv=3,\n",
    "    scoring='r2',\n",
    "    random_state=42,\n",
    "    verbose=2,\n",
    "    n_jobs=4\n",
    ")\n",
    "\n",
    "lgb_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\nBest LightGBM params: {lgb_search.best_params_}\")\n",
    "print(f\"Best CV R²: {lgb_search.best_score_:.4f}\")\n",
    "\n",
    "lgb_results, lgb_model = evaluate_model(lgb_search.best_estimator_, X_train, X_test,\n",
    "                                        y_train, y_test, \"LightGBM (Tuned)\")\n",
    "advanced_results.append(lgb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fac647",
   "metadata": {},
   "source": [
    "## Results Summary & Comparison to Steph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81540d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(advanced_results).sort_values('test_r2', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED MODELS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Compare to Steph\n",
    "best_r2 = results_df.iloc[0]['test_r2']\n",
    "steph_r2 = 0.884\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON TO STEPH'S BASELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Our Best:    {best_r2*100:.2f}% R² ({results_df.iloc[0]['model']})\")\n",
    "print(f\"Steph Best:  {steph_r2*100:.2f}% R² (Random Forest)\")\n",
    "print(f\"Difference:  {(best_r2 - steph_r2)*100:+.2f} percentage points\")\n",
    "\n",
    "if best_r2 > steph_r2:\n",
    "    print(f\"\\n🎉 SUCCESS! We BEAT Steph's baseline by {(best_r2 - steph_r2)*100:.2f}%!\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Still {(steph_r2 - best_r2)*100:.2f}% behind Steph. Consider:\")\n",
    "    print(\"   - Ensemble methods (stacking, blending)\")\n",
    "    print(\"   - Feature engineering improvements\")\n",
    "    print(\"   - More aggressive hyperparameter tuning\")\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(MODELS_DIR / 'advanced_models_results.csv', index=False)\n",
    "\n",
    "# Save best model\n",
    "best_model_name = results_df.iloc[0]['model']\n",
    "if 'Random Forest' in best_model_name:\n",
    "    best_model = rf_model\n",
    "elif 'Gradient Boosting' in best_model_name:\n",
    "    best_model = gb_model\n",
    "elif 'XGBoost' in best_model_name:\n",
    "    best_model = xgb_model\n",
    "else:\n",
    "    best_model = lgb_model\n",
    "\n",
    "joblib.dump(best_model, MODELS_DIR / 'best_advanced_model.joblib')\n",
    "\n",
    "# Save summary JSON\n",
    "summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_r2': float(best_r2),\n",
    "    'steph_r2': float(steph_r2),\n",
    "    'improvement': float(best_r2 - steph_r2),\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'n_features': X_train.shape[1],\n",
    "    'n_train_samples': X_train.shape[0],\n",
    "    'n_test_samples': X_test.shape[0]\n",
    "}\n",
    "\n",
    "with open(MODELS_DIR / 'advanced_models_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nModels saved to {MODELS_DIR}\")\n",
    "print(\"\\n✅ Advanced models training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
