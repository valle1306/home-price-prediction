{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300bd0b5",
   "metadata": {},
   "source": [
    "# 02_baseline_models.ipynb - Comprehensive Baseline Model Evaluation\n",
    "\n",
    "This notebook provides a systematic comparison of baseline machine learning models for house price prediction. We'll evaluate multiple algorithms using consistent metrics to establish performance benchmarks before moving to advanced methods.\n",
    "\n",
    "## Models Evaluated:\n",
    "1. **Linear Regression** - Basic linear relationship baseline\n",
    "2. **Ridge Regression** - L2 regularized linear model \n",
    "3. **Random Forest** - Tree-based ensemble method\n",
    "4. **Gradient Boosting** - Sequential boosting approach\n",
    "5. **Support Vector Regression** - Kernel-based regression\n",
    "6. **K-Nearest Neighbors** - Instance-based learning\n",
    "\n",
    "## Evaluation Metrics:\n",
    "- **R² Score** (Primary): Coefficient of determination \n",
    "- **RMSE**: Root Mean Square Error in dollars\n",
    "- **MAPE**: Mean Absolute Percentage Error\n",
    "- **MAE**: Mean Absolute Error in dollars\n",
    "\n",
    "**Input:** `data/cleaned_enhanced.csv` (from enhanced preprocessing)\n",
    "**Target:** ClosePrice prediction with comprehensive baseline comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_percentage_error, mean_absolute_error\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Baseline Models Evaluation - Setup Complete\")\n",
    "print(f\"Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Loading and Preparation\n",
    "ROOT = Path(r\"c:\\\\Users\\\\lpnhu\\\\Downloads\\\\home-price-prediction\")\n",
    "DATA_DIR = ROOT / 'data'\n",
    "ENHANCED_PATH = DATA_DIR / 'cleaned_enhanced.csv'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('=== LOADING ENHANCED DATASET ===')\n",
    "\n",
    "# Load the enhanced dataset\n",
    "if not ENHANCED_PATH.exists():\n",
    "    print(f'Enhanced dataset not found at {ENHANCED_PATH}')\n",
    "    print('Please run the 01_cleaning.ipynb notebook first to create the enhanced dataset.')\n",
    "    raise FileNotFoundError(f'Enhanced dataset not found: {ENHANCED_PATH}')\n",
    "\n",
    "data = pd.read_csv(ENHANCED_PATH)\n",
    "print(f'Loaded enhanced dataset: {data.shape}')\n",
    "print(f'Features available: {len(data.columns)} columns')\n",
    "print(f'Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB')\n",
    "\n",
    "# Prepare features and target\n",
    "exclude_columns = ['ClosePrice', 'ListingId', 'UnparsedAddress']\n",
    "feature_columns = [col for col in data.columns if col not in exclude_columns]\n",
    "\n",
    "X = data[feature_columns].copy()\n",
    "y = data['ClosePrice'].copy()\n",
    "\n",
    "# Handle any remaining missing values\n",
    "missing_before = X.isnull().sum().sum()\n",
    "if missing_before > 0:\n",
    "    print(f'Imputing {missing_before} missing values with median...')\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "print(f'\\n=== DATASET SUMMARY ===')\n",
    "print(f'Feature matrix: {X.shape}')\n",
    "print(f'Target variable: {y.shape}')\n",
    "print(f'Target range: ${y.min():,.0f} - ${y.max():,.0f}')\n",
    "print(f'Target mean: ${y.mean():,.0f}')\n",
    "print(f'Target median: ${y.median():,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6f66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Train/Test Split and Scaling Setup\n",
    "print('=== DATA SPLITTING AND SCALING ===')\n",
    "\n",
    "# Create consistent train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]:,} samples')\n",
    "print(f'Test set: {X_test.shape[0]:,} samples')\n",
    "\n",
    "# Prepare scalers for models that need normalized features\n",
    "standard_scaler = StandardScaler()\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit scalers on training data only\n",
    "X_train_std = standard_scaler.fit_transform(X_train)\n",
    "X_test_std = standard_scaler.transform(X_test)\n",
    "\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "\n",
    "print(f'Scaling complete - StandardScaler and MinMaxScaler fitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbbd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Evaluation Framework\n",
    "def evaluate_model(model, X_train_data, X_test_data, y_train, y_test, model_name, scaling_type=\"None\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with consistent metrics\n",
    "    \"\"\"\n",
    "    print(f'\\n=== {model_name} ===')\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = datetime.now()\n",
    "    model.fit(X_train_data, y_train)\n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train_data)\n",
    "    y_pred_test = model.predict(X_test_data)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Cross-validation R² score\n",
    "    cv_scores = cross_val_score(model, X_train_data, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'scaling': scaling_type,\n",
    "        'training_time': training_time,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2,\n",
    "        'cv_r2_mean': cv_mean,\n",
    "        'cv_r2_std': cv_std,\n",
    "        'rmse': test_rmse,\n",
    "        'mae': test_mae,\n",
    "        'mape': test_mape,\n",
    "        'model_object': model\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f'  Scaling: {scaling_type}')\n",
    "    print(f'  Training time: {training_time:.2f}s')\n",
    "    print(f'  R² (train): {train_r2:.4f}')\n",
    "    print(f'  R² (test):  {test_r2:.4f}')\n",
    "    print(f'  R² (CV):    {cv_mean:.4f} ± {cv_std:.4f}')\n",
    "    print(f'  RMSE:       ${test_rmse:,.0f}')\n",
    "    print(f'  MAE:        ${test_mae:,.0f}')\n",
    "    print(f'  MAPE:       {test_mape*100:.2f}%')\n",
    "    \n",
    "    # Check for overfitting\n",
    "    overfitting = train_r2 - test_r2\n",
    "    if overfitting > 0.1:\n",
    "        print(f'  ⚠ Potential overfitting: {overfitting:.3f} difference')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Initialize results storage\n",
    "baseline_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b7932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Baseline Model Evaluation\n",
    "\n",
    "print('=== BASELINE MODELS EVALUATION ===')\n",
    "\n",
    "# 1. Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr_results = evaluate_model(lr, X_train, X_test, y_train, y_test, \"Linear Regression\")\n",
    "baseline_results.append(lr_results)\n",
    "\n",
    "# 2. Ridge Regression (with scaling)\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "ridge_results = evaluate_model(ridge, X_train_std, X_test_std, y_train, y_test, \n",
    "                              \"Ridge Regression\", \"StandardScaler\")\n",
    "baseline_results.append(ridge_results)\n",
    "\n",
    "# 3. Lasso Regression (with scaling)\n",
    "lasso = Lasso(alpha=1.0, random_state=42, max_iter=2000)\n",
    "lasso_results = evaluate_model(lasso, X_train_std, X_test_std, y_train, y_test,\n",
    "                              \"Lasso Regression\", \"StandardScaler\") \n",
    "baseline_results.append(lasso_results)\n",
    "\n",
    "# 4. Elastic Net (with scaling)\n",
    "elastic = ElasticNet(alpha=1.0, l1_ratio=0.5, random_state=42, max_iter=2000)\n",
    "elastic_results = evaluate_model(elastic, X_train_std, X_test_std, y_train, y_test,\n",
    "                                \"Elastic Net\", \"StandardScaler\")\n",
    "baseline_results.append(elastic_results)\n",
    "\n",
    "# 5. Decision Tree\n",
    "dt = DecisionTreeRegressor(max_depth=10, min_samples_split=20, min_samples_leaf=10, random_state=42)\n",
    "dt_results = evaluate_model(dt, X_train, X_test, y_train, y_test, \"Decision Tree\")\n",
    "baseline_results.append(dt_results)\n",
    "\n",
    "# 6. Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=15, min_samples_split=10, \n",
    "                          min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
    "rf_results = evaluate_model(rf, X_train, X_test, y_train, y_test, \"Random Forest\")\n",
    "baseline_results.append(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5240c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Tree-Based and Instance-Based Models\n",
    "\n",
    "# 7. Gradient Boosting\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=6, \n",
    "                              min_samples_split=20, min_samples_leaf=10, \n",
    "                              subsample=0.8, random_state=42)\n",
    "gb_results = evaluate_model(gb, X_train, X_test, y_train, y_test, \"Gradient Boosting\")\n",
    "baseline_results.append(gb_results)\n",
    "\n",
    "# 8. Support Vector Regression (with scaling - smaller sample for speed)\n",
    "print(f'\\nNote: Using subset for SVR due to computational complexity...')\n",
    "n_svr_samples = min(10000, len(X_train))\n",
    "X_train_svr = X_train_std[:n_svr_samples]\n",
    "y_train_svr = y_train.iloc[:n_svr_samples]\n",
    "\n",
    "svr = SVR(kernel='rbf', C=100, gamma='scale', epsilon=0.1)\n",
    "svr_results = evaluate_model(svr, X_train_svr, X_test_std, y_train_svr, y_test,\n",
    "                            \"Support Vector Regression\", \"StandardScaler\")\n",
    "baseline_results.append(svr_results)\n",
    "\n",
    "# 9. K-Nearest Neighbors (with scaling)\n",
    "knn = KNeighborsRegressor(n_neighbors=10, weights='distance', n_jobs=-1)\n",
    "knn_results = evaluate_model(knn, X_train_minmax, X_test_minmax, y_train, y_test,\n",
    "                            \"K-Nearest Neighbors\", \"MinMaxScaler\")\n",
    "baseline_results.append(knn_results)\n",
    "\n",
    "print(f'\\nBaseline evaluation complete! {len(baseline_results)} models evaluated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2082ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Results Summary and Comparison\n",
    "\n",
    "print('=== BASELINE RESULTS SUMMARY ===')\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'Scaling': result['scaling'],\n",
    "        'R² (Test)': result['test_r2'],\n",
    "        'R² (CV)': result['cv_r2_mean'],\n",
    "        'CV Std': result['cv_r2_std'],\n",
    "        'RMSE ($)': result['rmse'],\n",
    "        'MAE ($)': result['mae'],\n",
    "        'MAPE (%)': result['mape'] * 100,\n",
    "        'Training Time (s)': result['training_time']\n",
    "    }\n",
    "    for result in baseline_results\n",
    "])\n",
    "\n",
    "# Sort by test R² score\n",
    "results_df = results_df.sort_values('R² (Test)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nModel Performance Ranking:\")\n",
    "print(results_df.round(4).to_string(index=False))\n",
    "\n",
    "# Identify best performing model\n",
    "best_model = results_df.iloc[0]\n",
    "print(f'\\nBest Baseline Model: {best_model[\"Model\"]}')\n",
    "print(f'  R² Score: {best_model[\"R² (Test)\"]:.4f}')\n",
    "print(f'  RMSE: ${best_model[\"RMSE ($)\"]:,.0f}')\n",
    "print(f'  MAPE: {best_model[\"MAPE (%)\"]:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769a67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Visualization and Analysis\n",
    "\n",
    "print('=== CREATING PERFORMANCE VISUALIZATIONS ===')\n",
    "\n",
    "# Set up the plotting area\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. R² Score Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = results_df['Model']\n",
    "r2_scores = results_df['R² (Test)']\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "\n",
    "bars1 = ax1.barh(models, r2_scores, color=colors)\n",
    "ax1.set_xlabel('R² Score')\n",
    "ax1.set_title('Model Performance - R² Score Comparison')\n",
    "ax1.set_xlim(0, max(r2_scores) * 1.1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars1):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.3f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# 2. RMSE Comparison\n",
    "ax2 = axes[0, 1]\n",
    "rmse_scores = results_df['RMSE ($)'] / 1000  # Convert to thousands\n",
    "bars2 = ax2.barh(models, rmse_scores, color=colors)\n",
    "ax2.set_xlabel('RMSE (Thousands $)')\n",
    "ax2.set_title('Model Performance - RMSE Comparison')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    width = bar.get_width()\n",
    "    ax2.text(width + 5, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.0f}K', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# 3. Training Time vs Performance\n",
    "ax3 = axes[1, 0]\n",
    "training_times = results_df['Training Time (s)']\n",
    "scatter = ax3.scatter(training_times, r2_scores, c=range(len(models)), \n",
    "                     s=100, cmap='viridis', alpha=0.7)\n",
    "\n",
    "# Add model name annotations\n",
    "for i, model in enumerate(models):\n",
    "    ax3.annotate(model, (training_times.iloc[i], r2_scores.iloc[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9, alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Training Time (seconds)')\n",
    "ax3.set_ylabel('R² Score')\n",
    "ax3.set_title('Training Time vs Performance')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Cross-Validation Results\n",
    "ax4 = axes[1, 1]\n",
    "cv_means = results_df['R² (CV)']\n",
    "cv_stds = results_df['CV Std']\n",
    "\n",
    "bars4 = ax4.barh(models, cv_means, xerr=cv_stds, color=colors, \n",
    "                capsize=5, error_kw={'capthick': 2})\n",
    "ax4.set_xlabel('Cross-Validation R² Score')\n",
    "ax4.set_title('Cross-Validation Results (Mean ± Std)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the visualization\n",
    "plot_path = MODELS_DIR / 'baseline_models_comparison.png'\n",
    "plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f'Comparison plot saved to: {plot_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Save Results and Best Model\n",
    "\n",
    "print('=== SAVING RESULTS AND BEST MODEL ===')\n",
    "\n",
    "# Save detailed results\n",
    "baseline_summary = {\n",
    "    'evaluation_timestamp': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(data),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'features_count': len(feature_columns),\n",
    "        'target_variable': 'ClosePrice'\n",
    "    },\n",
    "    'models_evaluated': len(baseline_results),\n",
    "    'best_model': {\n",
    "        'name': best_model['Model'],\n",
    "        'r2_score': float(best_model['R² (Test)']),\n",
    "        'rmse': float(best_model['RMSE ($)']),\n",
    "        'mae': float(best_model['MAE ($)']),\n",
    "        'mape': float(best_model['MAPE (%)']) / 100,\n",
    "        'training_time': float(best_model['Training Time (s)'])\n",
    "    },\n",
    "    'all_results': [\n",
    "        {\n",
    "            'model_name': result['model_name'],\n",
    "            'scaling': result['scaling'],\n",
    "            'test_r2': float(result['test_r2']),\n",
    "            'cv_r2_mean': float(result['cv_r2_mean']),\n",
    "            'cv_r2_std': float(result['cv_r2_std']),\n",
    "            'rmse': float(result['rmse']),\n",
    "            'mae': float(result['mae']),\n",
    "            'mape': float(result['mape']),\n",
    "            'training_time': float(result['training_time'])\n",
    "        }\n",
    "        for result in baseline_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "summary_path = MODELS_DIR / 'baseline_models_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(baseline_summary, f, indent=2)\n",
    "\n",
    "# Save the best model\n",
    "best_model_obj = next(result['model_object'] for result in baseline_results \n",
    "                     if result['model_name'] == best_model['Model'])\n",
    "\n",
    "best_model_path = MODELS_DIR / f'best_baseline_{best_model[\"Model\"].lower().replace(\" \", \"_\")}.joblib'\n",
    "joblib.dump(best_model_obj, best_model_path)\n",
    "\n",
    "# Save results DataFrame\n",
    "results_csv_path = MODELS_DIR / 'baseline_models_results.csv'\n",
    "results_df.to_csv(results_csv_path, index=False)\n",
    "\n",
    "print(f'Summary saved to: {summary_path}')\n",
    "print(f'Best model saved to: {best_model_path}')\n",
    "print(f'Results table saved to: {results_csv_path}')\n",
    "\n",
    "print(f'\\n=== BASELINE EVALUATION COMPLETE ===')\n",
    "print(f'Best performing model: {best_model[\"Model\"]}')\n",
    "print(f'Achieved R² = {best_model[\"R² (Test)\"]:.4f}')\n",
    "print(f'Target for advanced methods: > {best_model[\"R² (Test)\"]:.4f}')\n",
    "print(f'\\nNext steps:')\n",
    "print(f'  1. Run 03_modeling_baseline.ipynb for optimized XGBoost')\n",
    "print(f'  2. Run 04_advanced_methods.ipynb for deep learning approaches')\n",
    "print(f'  3. Compare results across all notebooks')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
