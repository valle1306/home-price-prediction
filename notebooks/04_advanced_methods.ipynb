{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e40063",
   "metadata": {},
   "source": [
    "# 04_advanced_methods.ipynb - Advanced ML/DL Techniques & Feature Engineering\n",
    "\n",
    "This notebook explores cutting-edge machine learning and deep learning approaches for house price prediction, including advanced feature engineering techniques and sophisticated model architectures.\n",
    "\n",
    "## Advanced Models:\n",
    "1. **LightGBM** - High-performance gradient boosting\n",
    "2. **CatBoost** - Categorical feature-optimized boosting  \n",
    "3. **Neural Networks** - Deep learning with TensorFlow/Keras\n",
    "4. **Ensemble Methods** - Stacking and blending approaches\n",
    "5. **AutoML** - Automated feature selection and hyperparameter tuning\n",
    "\n",
    "## Advanced Feature Engineering:\n",
    "1. **Polynomial Features** - Non-linear feature interactions\n",
    "2. **Geographic Features** - Location-based engineered features\n",
    "3. **Time-based Features** - Temporal patterns and seasonality\n",
    "4. **Target Encoding Variants** - Advanced categorical encoding\n",
    "5. **Feature Selection** - Automated important feature identification\n",
    "\n",
    "## Evaluation Framework:\n",
    "- **Comprehensive Metrics**: R², RMSE, MAPE, MAE\n",
    "- **Cross-Validation**: Stratified and time-based splits\n",
    "- **Model Interpretability**: SHAP values and feature importance\n",
    "- **Performance Profiling**: Training time and prediction speed\n",
    "\n",
    "**Target:** Exceed XGBoost baseline (R² = 0.8972) with advanced techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e34f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Advanced Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced ML imports\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    print(f'LightGBM version: {lgb.__version__}')\n",
    "except ImportError:\n",
    "    print('Installing LightGBM...')\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'lightgbm'])\n",
    "    import lightgbm as lgb\n",
    "\n",
    "try:\n",
    "    import catboost as cb\n",
    "    print(f'CatBoost version: {cb.__version__}')\n",
    "except ImportError:\n",
    "    print('Installing CatBoost...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'catboost'])\n",
    "    import catboost as cb\n",
    "\n",
    "# Deep Learning imports\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, callbacks\n",
    "    print(f'TensorFlow version: {tf.__version__}')\n",
    "except ImportError:\n",
    "    print('Installing TensorFlow...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow'])\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, callbacks\n",
    "\n",
    "# Advanced preprocessing\n",
    "from sklearn.preprocessing import PolynomialFeatures, PowerTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.ensemble import VotingRegressor, StackingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Other advanced tools\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import json\n",
    "from scipy import stats\n",
    "\n",
    "print(f\"Advanced Methods Setup Complete - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ee73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Load Data and Baseline Results\n",
    "ROOT = Path(r\"c:\\\\Users\\\\lpnhu\\\\Downloads\\\\home-price-prediction\")\n",
    "DATA_DIR = ROOT / 'data'\n",
    "ENHANCED_PATH = DATA_DIR / 'cleaned_enhanced.csv'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "\n",
    "print('=== LOADING DATA AND BASELINE RESULTS ===')\n",
    "\n",
    "# Load enhanced dataset\n",
    "data = pd.read_csv(ENHANCED_PATH)\n",
    "print(f'Enhanced dataset loaded: {data.shape}')\n",
    "\n",
    "# Load baseline results for comparison\n",
    "try:\n",
    "    baseline_summary_path = MODELS_DIR / 'baseline_models_summary.json'\n",
    "    with open(baseline_summary_path, 'r') as f:\n",
    "        baseline_summary = json.load(f)\n",
    "    \n",
    "    baseline_best_r2 = baseline_summary['best_model']['r2_score']\n",
    "    baseline_best_model = baseline_summary['best_model']['name']\n",
    "    print(f'Baseline best model: {baseline_best_model} (R² = {baseline_best_r2:.4f})')\n",
    "except FileNotFoundError:\n",
    "    baseline_best_r2 = 0.85  # Conservative baseline\n",
    "    baseline_best_model = \"Unknown\"\n",
    "    print(f'Baseline results not found, using conservative target: R² = {baseline_best_r2:.4f}')\n",
    "\n",
    "# Load XGBoost results for comparison\n",
    "try:\n",
    "    xgb_metrics_path = MODELS_DIR / 'xgboost_enhanced_metrics.json'\n",
    "    with open(xgb_metrics_path, 'r') as f:\n",
    "        xgb_metrics = json.load(f)\n",
    "    \n",
    "    xgb_r2 = xgb_metrics['test_metrics']['r2']\n",
    "    print(f'XGBoost enhanced R²: {xgb_r2:.4f}')\n",
    "    target_r2 = max(baseline_best_r2, xgb_r2)\n",
    "except FileNotFoundError:\n",
    "    target_r2 = baseline_best_r2\n",
    "    print(f'XGBoost results not found')\n",
    "\n",
    "print(f'Target to exceed: R² > {target_r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Advanced Feature Engineering\n",
    "print('=== ADVANCED FEATURE ENGINEERING ===')\n",
    "\n",
    "# Prepare base features\n",
    "exclude_columns = ['ClosePrice', 'ListingId', 'UnparsedAddress']\n",
    "base_features = [col for col in data.columns if col not in exclude_columns]\n",
    "\n",
    "X_base = data[base_features].copy()\n",
    "y = data['ClosePrice'].copy()\n",
    "\n",
    "# Handle missing values\n",
    "X_base = X_base.fillna(X_base.median())\n",
    "print(f'Base features: {X_base.shape[1]}')\n",
    "\n",
    "# 1. Geographic Clustering Features\n",
    "print('Creating geographic clustering features...')\n",
    "if 'Latitude' in X_base.columns and 'Longitude' in X_base.columns:\n",
    "    coords = X_base[['Latitude', 'Longitude']].fillna(X_base[['Latitude', 'Longitude']].median())\n",
    "    \n",
    "    # K-means clustering for geographic regions\n",
    "    for n_clusters in [5, 10, 20]:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        X_base[f'GeoCluster_{n_clusters}'] = kmeans.fit_predict(coords)\n",
    "    \n",
    "    # Distance from city center (assuming center is median lat/lon)\n",
    "    center_lat, center_lon = coords.median()\n",
    "    X_base['DistanceFromCenter'] = np.sqrt(\n",
    "        (coords['Latitude'] - center_lat)**2 + \n",
    "        (coords['Longitude'] - center_lon)**2\n",
    "    )\n",
    "    print('Geographic features created')\n",
    "\n",
    "# 2. Advanced Time Features  \n",
    "print('Creating advanced temporal features...')\n",
    "if 'BuildingAge' in X_base.columns:\n",
    "    # Age-based binning\n",
    "    X_base['AgeCategory'] = pd.cut(X_base['BuildingAge'], \n",
    "                                  bins=[0, 5, 15, 30, 50, 100], \n",
    "                                  labels=['New', 'Recent', 'Mature', 'Old', 'Historic'])\n",
    "    \n",
    "    # Age squared for non-linear effects\n",
    "    X_base['BuildingAge_Squared'] = X_base['BuildingAge'] ** 2\n",
    "    X_base['BuildingAge_Log'] = np.log1p(X_base['BuildingAge'])\n",
    "\n",
    "# 3. Property Size Interactions\n",
    "print('Creating property size interaction features...')\n",
    "size_columns = [col for col in X_base.columns if any(keyword in col.lower() \n",
    "               for keyword in ['sqft', 'size', 'area', 'room', 'bed', 'bath'])]\n",
    "\n",
    "if len(size_columns) >= 2:\n",
    "    # Create ratios and interactions for top size features\n",
    "    for i in range(min(3, len(size_columns))):\n",
    "        for j in range(i+1, min(3, len(size_columns))):\n",
    "            col1, col2 = size_columns[i], size_columns[j]\n",
    "            # Ratio features\n",
    "            X_base[f'{col1}_{col2}_ratio'] = X_base[col1] / (X_base[col2] + 1)\n",
    "            # Product features  \n",
    "            X_base[f'{col1}_{col2}_product'] = X_base[col1] * X_base[col2]\n",
    "\n",
    "# 4. Statistical Features per Categorical Group\n",
    "print('Creating statistical group features...')\n",
    "categorical_cols = X_base.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X_base.select_dtypes(include=[np.number]).columns[:10]  # Top 10 numerical\n",
    "\n",
    "for cat_col in categorical_cols[:3]:  # Limit to prevent explosion\n",
    "    if X_base[cat_col].nunique() < 50:  # Only for reasonable cardinality\n",
    "        for num_col in numerical_cols[:3]:\n",
    "            group_stats = X_base.groupby(cat_col)[num_col].agg(['mean', 'std', 'median'])\n",
    "            X_base[f'{cat_col}_{num_col}_mean'] = X_base[cat_col].map(group_stats['mean'])\n",
    "            X_base[f'{cat_col}_{num_col}_std'] = X_base[cat_col].map(group_stats['std'])\n",
    "\n",
    "print(f'Feature engineering complete. Total features: {X_base.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e58e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Feature Selection and Preprocessing\n",
    "print('=== FEATURE SELECTION AND PREPROCESSING ===')\n",
    "\n",
    "# Convert categorical variables to numerical using target encoding\n",
    "categorical_columns = X_base.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if X_base[col].nunique() < 100:  # Reasonable cardinality\n",
    "        # Target encoding with smoothing\n",
    "        target_mean = y.mean()\n",
    "        counts = X_base[col].value_counts()\n",
    "        means = y.groupby(X_base[col]).mean()\n",
    "        \n",
    "        # Smoothing factor (higher = more smoothing)\n",
    "        smooth = 10\n",
    "        smoothed_means = (counts * means + smooth * target_mean) / (counts + smooth)\n",
    "        \n",
    "        X_base[f'{col}_target_encoded'] = X_base[col].map(smoothed_means)\n",
    "        X_base[f'{col}_count'] = X_base[col].map(counts)\n",
    "\n",
    "# Drop original categorical columns\n",
    "X_processed = X_base.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "# Handle any infinite values\n",
    "X_processed = X_processed.replace([np.inf, -np.inf], np.nan)\n",
    "X_processed = X_processed.fillna(X_processed.median())\n",
    "\n",
    "print(f'Processed feature matrix: {X_processed.shape}')\n",
    "\n",
    "# Feature selection using mutual information\n",
    "print('Performing feature selection...')\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Calculate feature importance\n",
    "mi_scores = mutual_info_regression(X_processed, y, random_state=42)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_processed.columns,\n",
    "    'importance': mi_scores\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Select top features (adaptive based on total features)\n",
    "n_features = min(200, int(X_processed.shape[1] * 0.8))  \n",
    "top_features = feature_importance.head(n_features)['feature'].tolist()\n",
    "\n",
    "X_selected = X_processed[top_features].copy()\n",
    "print(f'Selected {len(top_features)} most important features')\n",
    "print(f'Final feature matrix: {X_selected.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32d7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Advanced Train/Test Split and Evaluation Framework\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('=== ADVANCED DATA SPLITTING ===')\n",
    "\n",
    "# Advanced train/validation/test split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, shuffle=True  # 0.25 of 0.8 = 0.2 of total\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train.shape[0]:,} samples')\n",
    "print(f'Validation set: {X_val.shape[0]:,} samples') \n",
    "print(f'Test set: {X_test.shape[0]:,} samples')\n",
    "\n",
    "# Advanced evaluation function\n",
    "def advanced_evaluate_model(model, X_train, X_val, X_test, y_train, y_val, y_test, \n",
    "                          model_name, fit_kwargs=None):\n",
    "    \"\"\"Advanced model evaluation with validation set\"\"\"\n",
    "    print(f'\\n=== {model_name} ===')\n",
    "    \n",
    "    fit_kwargs = fit_kwargs or {}\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Train model\n",
    "    if 'eval_set' in fit_kwargs:\n",
    "        model.fit(X_train, y_train, **fit_kwargs)\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val) \n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    val_r2 = r2_score(y_val, y_pred_val)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    test_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'train_r2': train_r2,\n",
    "        'val_r2': val_r2, \n",
    "        'test_r2': test_r2,\n",
    "        'rmse': test_rmse,\n",
    "        'mae': test_mae,\n",
    "        'mape': test_mape,\n",
    "        'training_time': training_time,\n",
    "        'model_object': model,\n",
    "        'predictions': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f'  Training time: {training_time:.2f}s')\n",
    "    print(f'  R² (train): {train_r2:.4f}')\n",
    "    print(f'  R² (val):   {val_r2:.4f}') \n",
    "    print(f'  R² (test):  {test_r2:.4f}')\n",
    "    print(f'  RMSE:       ${test_rmse:,.0f}')\n",
    "    print(f'  MAE:        ${test_mae:,.0f}')\n",
    "    print(f'  MAPE:       {test_mape*100:.2f}%')\n",
    "    \n",
    "    # Overfitting check\n",
    "    if train_r2 - val_r2 > 0.05:\n",
    "        print(f'  ⚠ Potential overfitting detected')\n",
    "    \n",
    "    return results\n",
    "\n",
    "advanced_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: LightGBM Implementation\n",
    "print('=== LIGHTGBM ADVANCED IMPLEMENTATION ===')\n",
    "\n",
    "# LightGBM with early stopping and validation\n",
    "lgb_params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'min_child_samples': 20,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=2000, **lgb_params)\n",
    "\n",
    "# Use validation set for early stopping\n",
    "fit_kwargs_lgb = {\n",
    "    'eval_set': [(X_val, y_val)],\n",
    "    'callbacks': [lgb.early_stopping(100), lgb.log_evaluation(0)]\n",
    "}\n",
    "\n",
    "lgb_results = advanced_evaluate_model(\n",
    "    lgb_model, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    \"LightGBM\", fit_kwargs_lgb\n",
    ")\n",
    "advanced_results.append(lgb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f5e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: CatBoost Implementation  \n",
    "print('=== CATBOOST ADVANCED IMPLEMENTATION ===')\n",
    "\n",
    "# CatBoost with built-in categorical handling\n",
    "catboost_model = cb.CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.01,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=3,\n",
    "    subsample=0.8,\n",
    "    colsample_bylevel=0.8,\n",
    "    random_seed=42,\n",
    "    logging_level='Silent',\n",
    "    use_best_model=True,\n",
    "    eval_metric='RMSE'\n",
    ")\n",
    "\n",
    "fit_kwargs_cb = {\n",
    "    'eval_set': (X_val, y_val),\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "cb_results = advanced_evaluate_model(\n",
    "    catboost_model, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    \"CatBoost\", fit_kwargs_cb\n",
    ")\n",
    "advanced_results.append(cb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f25439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Neural Network Implementation\n",
    "print('=== NEURAL NETWORK IMPLEMENTATION ===')\n",
    "\n",
    "# Preprocessing for neural networks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_scaled = scaler_nn.fit_transform(X_train)\n",
    "X_val_scaled = scaler_nn.transform(X_val)\n",
    "X_test_scaled = scaler_nn.transform(X_test)\n",
    "\n",
    "# Build neural network architecture\n",
    "def create_neural_network(input_dim):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(512, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(), \n",
    "        layers.Dropout(0.3),\n",
    "        \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        \n",
    "        layers.Dense(32, activation='relu'),\n",
    "        layers.Dense(1)  # Output layer\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train neural network\n",
    "nn_model = create_neural_network(X_train_scaled.shape[1])\n",
    "\n",
    "# Callbacks for training\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7\n",
    ")\n",
    "\n",
    "print('Training Neural Network...')\n",
    "history = nn_model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_data=(X_val_scaled, y_val),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f'Training completed after {len(history.history[\"loss\"])} epochs')\n",
    "\n",
    "# Evaluate neural network\n",
    "y_pred_nn_test = nn_model.predict(X_test_scaled, verbose=0).flatten()\n",
    "y_pred_nn_val = nn_model.predict(X_val_scaled, verbose=0).flatten()\n",
    "y_pred_nn_train = nn_model.predict(X_train_scaled, verbose=0).flatten()\n",
    "\n",
    "nn_results = {\n",
    "    'model_name': 'Neural Network',\n",
    "    'train_r2': r2_score(y_train, y_pred_nn_train),\n",
    "    'val_r2': r2_score(y_val, y_pred_nn_val),\n",
    "    'test_r2': r2_score(y_test, y_pred_nn_test),\n",
    "    'rmse': np.sqrt(mean_squared_error(y_test, y_pred_nn_test)),\n",
    "    'mae': mean_absolute_error(y_test, y_pred_nn_test),\n",
    "    'mape': mean_absolute_percentage_error(y_test, y_pred_nn_test),\n",
    "    'training_time': 0,  # Approximate \n",
    "    'model_object': nn_model,\n",
    "    'predictions': y_pred_nn_test,\n",
    "    'scaler': scaler_nn\n",
    "}\n",
    "\n",
    "print(f\"Neural Network Results:\")\n",
    "print(f\"  R² (train): {nn_results['train_r2']:.4f}\")\n",
    "print(f\"  R² (val):   {nn_results['val_r2']:.4f}\")\n",
    "print(f\"  R² (test):  {nn_results['test_r2']:.4f}\")\n",
    "print(f\"  RMSE:       ${nn_results['rmse']:,.0f}\")\n",
    "\n",
    "advanced_results.append(nn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e876bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Ensemble Methods\n",
    "print('=== ENSEMBLE METHODS ===')\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Create ensemble of best models\n",
    "print('Creating ensemble model...')\n",
    "\n",
    "# Use the trained models (without refitting)\n",
    "ensemble_models = [\n",
    "    ('lightgbm', lgb_results['model_object']),\n",
    "    ('catboost', cb_results['model_object']),\n",
    "    ('ridge', Ridge(alpha=1.0))  # Add a simple linear model for diversity\n",
    "]\n",
    "\n",
    "# Fit the ridge model\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train, y_train)\n",
    "ensemble_models[2] = ('ridge', ridge_model)\n",
    "\n",
    "# Create voting ensemble\n",
    "voting_ensemble = VotingRegressor(ensemble_models)\n",
    "voting_ensemble.fit(X_train, y_train)  # This won't refit already fitted models\n",
    "\n",
    "voting_results = advanced_evaluate_model(\n",
    "    voting_ensemble, X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    \"Voting Ensemble\"\n",
    ")\n",
    "advanced_results.append(voting_results)\n",
    "\n",
    "# Weighted ensemble (manual)\n",
    "print('Creating weighted ensemble...')\n",
    "\n",
    "lgb_pred = lgb_results['model_object'].predict(X_test)\n",
    "cb_pred = cb_results['model_object'].predict(X_test)\n",
    "nn_pred = y_pred_nn_test\n",
    "\n",
    "# Optimal weights based on validation performance\n",
    "val_scores = [lgb_results['val_r2'], cb_results['val_r2'], nn_results['val_r2']]\n",
    "weights = np.array(val_scores) / sum(val_scores)  # Normalize to sum to 1\n",
    "\n",
    "weighted_pred = (weights[0] * lgb_pred + \n",
    "                weights[1] * cb_pred + \n",
    "                weights[2] * nn_pred)\n",
    "\n",
    "weighted_r2 = r2_score(y_test, weighted_pred)\n",
    "weighted_rmse = np.sqrt(mean_squared_error(y_test, weighted_pred))\n",
    "weighted_mape = mean_absolute_percentage_error(y_test, weighted_pred)\n",
    "\n",
    "print(f\"\\nWeighted Ensemble Results:\")\n",
    "print(f\"  Weights: LightGBM={weights[0]:.3f}, CatBoost={weights[1]:.3f}, NN={weights[2]:.3f}\")\n",
    "print(f\"  R² (test):  {weighted_r2:.4f}\")\n",
    "print(f\"  RMSE:       ${weighted_rmse:,.0f}\")\n",
    "print(f\"  MAPE:       {weighted_mape*100:.2f}%\")\n",
    "\n",
    "weighted_results = {\n",
    "    'model_name': 'Weighted Ensemble',\n",
    "    'train_r2': None, \n",
    "    'val_r2': None,\n",
    "    'test_r2': weighted_r2,\n",
    "    'rmse': weighted_rmse,\n",
    "    'mae': mean_absolute_error(y_test, weighted_pred),\n",
    "    'mape': weighted_mape,\n",
    "    'training_time': 0,\n",
    "    'model_object': None,\n",
    "    'predictions': weighted_pred,\n",
    "    'weights': weights\n",
    "}\n",
    "\n",
    "advanced_results.append(weighted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29ca8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Results Analysis and Comparison\n",
    "print('=== ADVANCED METHODS RESULTS ANALYSIS ===')\n",
    "\n",
    "# Create comprehensive results DataFrame\n",
    "results_df_advanced = pd.DataFrame([\n",
    "    {\n",
    "        'Model': result['model_name'],\n",
    "        'R² (Train)': result.get('train_r2', np.nan),\n",
    "        'R² (Val)': result.get('val_r2', np.nan),\n",
    "        'R² (Test)': result['test_r2'],\n",
    "        'RMSE ($)': result['rmse'],\n",
    "        'MAE ($)': result['mae'], \n",
    "        'MAPE (%)': result['mape'] * 100,\n",
    "        'Training Time (s)': result['training_time']\n",
    "    }\n",
    "    for result in advanced_results\n",
    "])\n",
    "\n",
    "# Sort by test R² score\n",
    "results_df_advanced = results_df_advanced.sort_values('R² (Test)', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nAdvanced Methods Performance Ranking:\")\n",
    "print(results_df_advanced.round(4).to_string(index=False))\n",
    "\n",
    "# Compare with previous best results\n",
    "best_advanced = results_df_advanced.iloc[0]\n",
    "print(f'\\nBest Advanced Model: {best_advanced[\"Model\"]}')\n",
    "print(f'  R² Score: {best_advanced[\"R² (Test)\"]:.4f}')\n",
    "print(f'  RMSE: ${best_advanced[\"RMSE ($)\"]:,.0f}')\n",
    "print(f'  MAPE: {best_advanced[\"MAPE (%)\"]:.2f}%')\n",
    "\n",
    "print(f'\\n=== PERFORMANCE COMPARISON ===')\n",
    "print(f'Target to exceed: {target_r2:.4f}')\n",
    "print(f'Best advanced:    {best_advanced[\"R² (Test)\"]:.4f}')\n",
    "\n",
    "improvement = best_advanced[\"R² (Test)\"] - target_r2\n",
    "if improvement > 0:\n",
    "    print(f'✓ IMPROVEMENT ACHIEVED: +{improvement:.4f} R² points')\n",
    "else:\n",
    "    print(f'✗ Target not exceeded: {improvement:.4f} R² points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f8789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Advanced Visualizations and Model Interpretation\n",
    "print('=== ADVANCED VISUALIZATIONS AND INTERPRETABILITY ===')\n",
    "\n",
    "# Comprehensive performance visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. Model Performance Comparison\n",
    "ax1 = axes[0, 0]\n",
    "models = results_df_advanced['Model']\n",
    "r2_scores = results_df_advanced['R² (Test)']\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "\n",
    "bars = ax1.barh(models, r2_scores, color=colors)\n",
    "ax1.axvline(x=target_r2, color='red', linestyle='--', alpha=0.7, label=f'Target: {target_r2:.3f}')\n",
    "ax1.set_xlabel('R² Score')\n",
    "ax1.set_title('Advanced Methods - R² Score Comparison')\n",
    "ax1.legend()\n",
    "\n",
    "for i, bar in enumerate(bars):\n",
    "    width = bar.get_width()\n",
    "    ax1.text(width + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{width:.4f}', ha='left', va='center', fontsize=10)\n",
    "\n",
    "# 2. RMSE vs R² scatter\n",
    "ax2 = axes[0, 1]\n",
    "rmse_values = results_df_advanced['RMSE ($)'] / 1000\n",
    "scatter = ax2.scatter(r2_scores, rmse_values, c=range(len(models)), \n",
    "                     s=150, cmap='viridis', alpha=0.7, edgecolors='black')\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    ax2.annotate(model, (r2_scores.iloc[i], rmse_values.iloc[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax2.set_xlabel('R² Score')\n",
    "ax2.set_ylabel('RMSE (Thousands $)')\n",
    "ax2.set_title('R² vs RMSE Trade-off')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Training/Validation/Test R² comparison\n",
    "ax3 = axes[0, 2]\n",
    "train_r2 = results_df_advanced['R² (Train)'].fillna(0)\n",
    "val_r2 = results_df_advanced['R² (Val)'].fillna(0)\n",
    "test_r2 = results_df_advanced['R² (Test)']\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "ax3.bar(x - width, train_r2, width, label='Train', alpha=0.8)\n",
    "ax3.bar(x, val_r2, width, label='Validation', alpha=0.8) \n",
    "ax3.bar(x + width, test_r2, width, label='Test', alpha=0.8)\n",
    "\n",
    "ax3.set_xlabel('Models')\n",
    "ax3.set_ylabel('R² Score')\n",
    "ax3.set_title('Train/Validation/Test R² Comparison')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Best model predictions scatter plot\n",
    "ax4 = axes[1, 0]\n",
    "best_model_name = best_advanced['Model']\n",
    "best_predictions = None\n",
    "\n",
    "for result in advanced_results:\n",
    "    if result['model_name'] == best_model_name:\n",
    "        best_predictions = result['predictions']\n",
    "        break\n",
    "\n",
    "if best_predictions is not None:\n",
    "    scatter = ax4.scatter(y_test, best_predictions, alpha=0.6, c='blue', edgecolors='none')\n",
    "    ax4.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax4.set_xlabel('Actual ClosePrice')\n",
    "    ax4.set_ylabel('Predicted ClosePrice')\n",
    "    ax4.set_title(f'{best_model_name}: Actual vs Predicted')\n",
    "    \n",
    "    # Add R² text\n",
    "    ax4.text(0.05, 0.95, f'R² = {best_advanced[\"R² (Test)\"]:.4f}', \n",
    "            transform=ax4.transAxes, fontsize=12, verticalalignment='top',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 5. Feature importance (for tree-based models)\n",
    "ax5 = axes[1, 1]\n",
    "try:\n",
    "    # Try to get feature importance from LightGBM\n",
    "    lgb_model_obj = lgb_results['model_object']\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': X_selected.columns,\n",
    "        'importance': lgb_model_obj.feature_importances_\n",
    "    }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    ax5.barh(range(len(feature_imp)), feature_imp['importance'])\n",
    "    ax5.set_yticks(range(len(feature_imp)))\n",
    "    ax5.set_yticklabels(feature_imp['feature'])\n",
    "    ax5.set_xlabel('Feature Importance')\n",
    "    ax5.set_title('Top 15 Feature Importances (LightGBM)')\n",
    "    ax5.invert_yaxis()\n",
    "except Exception as e:\n",
    "    ax5.text(0.5, 0.5, 'Feature importance\\nnot available', \n",
    "            ha='center', va='center', transform=ax5.transAxes)\n",
    "    ax5.set_title('Feature Importance')\n",
    "\n",
    "# 6. Model complexity vs performance\n",
    "ax6 = axes[1, 2]\n",
    "model_complexity = {\n",
    "    'Neural Network': 4,  # Deep model\n",
    "    'LightGBM': 3,       # Gradient boosting\n",
    "    'CatBoost': 3,       # Gradient boosting  \n",
    "    'Voting Ensemble': 3.5,  # Ensemble\n",
    "    'Weighted Ensemble': 4.5  # Complex ensemble\n",
    "}\n",
    "\n",
    "complexity_scores = [model_complexity.get(model, 2) for model in models]\n",
    "ax6.scatter(complexity_scores, r2_scores, s=150, c=colors, alpha=0.7)\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    ax6.annotate(model, (complexity_scores[i], r2_scores.iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax6.set_xlabel('Model Complexity')\n",
    "ax6.set_ylabel('R² Score') \n",
    "ax6.set_title('Model Complexity vs Performance')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save visualization\n",
    "advanced_plot_path = MODELS_DIR / 'advanced_methods_analysis.png'\n",
    "plt.savefig(advanced_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f'Advanced analysis plot saved to: {advanced_plot_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4fab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12: Save Advanced Results and Models\n",
    "print('=== SAVING ADVANCED RESULTS AND MODELS ===')\n",
    "\n",
    "# Compile comprehensive results\n",
    "advanced_summary = {\n",
    "    'evaluation_timestamp': datetime.now().isoformat(),\n",
    "    'dataset_info': {\n",
    "        'total_samples': len(data),\n",
    "        'train_samples': len(X_train), \n",
    "        'val_samples': len(X_val),\n",
    "        'test_samples': len(X_test),\n",
    "        'features_engineered': X_selected.shape[1],\n",
    "        'target_variable': 'ClosePrice'\n",
    "    },\n",
    "    'baseline_comparison': {\n",
    "        'previous_best_r2': target_r2,\n",
    "        'advanced_best_r2': float(best_advanced['R² (Test)']),\n",
    "        'improvement': float(best_advanced['R² (Test)'] - target_r2)\n",
    "    },\n",
    "    'best_model': {\n",
    "        'name': best_advanced['Model'],\n",
    "        'r2_score': float(best_advanced['R² (Test)']),\n",
    "        'rmse': float(best_advanced['RMSE ($)']),\n",
    "        'mae': float(best_advanced['MAE ($)']),\n",
    "        'mape': float(best_advanced['MAPE (%)']) / 100\n",
    "    },\n",
    "    'all_results': [\n",
    "        {\n",
    "            'model_name': result['model_name'],\n",
    "            'test_r2': float(result['test_r2']),\n",
    "            'val_r2': float(result.get('val_r2', 0)),\n",
    "            'rmse': float(result['rmse']),\n",
    "            'mae': float(result['mae']),\n",
    "            'mape': float(result['mape']),\n",
    "            'training_time': float(result['training_time'])\n",
    "        }\n",
    "        for result in advanced_results\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "advanced_summary_path = MODELS_DIR / 'advanced_methods_summary.json'\n",
    "with open(advanced_summary_path, 'w') as f:\n",
    "    json.dump(advanced_summary, f, indent=2)\n",
    "\n",
    "# Save the best model\n",
    "best_model_result = next(result for result in advanced_results \n",
    "                        if result['model_name'] == best_advanced['Model'])\n",
    "\n",
    "if best_model_result['model_object'] is not None:\n",
    "    if best_advanced['Model'] == 'Neural Network':\n",
    "        # Save neural network model\n",
    "        nn_model_path = MODELS_DIR / 'best_neural_network.h5'\n",
    "        best_model_result['model_object'].save(nn_model_path)\n",
    "        \n",
    "        # Save scaler\n",
    "        scaler_path = MODELS_DIR / 'neural_network_scaler.joblib'\n",
    "        joblib.dump(best_model_result['scaler'], scaler_path)\n",
    "        print(f'Neural network saved to: {nn_model_path}')\n",
    "        print(f'Scaler saved to: {scaler_path}')\n",
    "    else:\n",
    "        # Save other models\n",
    "        best_model_path = MODELS_DIR / f'best_advanced_{best_advanced[\"Model\"].lower().replace(\" \", \"_\")}.joblib'\n",
    "        joblib.dump(best_model_result['model_object'], best_model_path)\n",
    "        print(f'Best model saved to: {best_model_path}')\n",
    "\n",
    "# Save results DataFrame\n",
    "results_csv_path = MODELS_DIR / 'advanced_methods_results.csv'\n",
    "results_df_advanced.to_csv(results_csv_path, index=False)\n",
    "\n",
    "# Save feature names for reproducibility\n",
    "feature_names_path = MODELS_DIR / 'advanced_feature_names.json'\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    json.dump(X_selected.columns.tolist(), f, indent=2)\n",
    "\n",
    "print(f'Advanced summary saved to: {advanced_summary_path}')\n",
    "print(f'Results table saved to: {results_csv_path}')\n",
    "print(f'Feature names saved to: {feature_names_path}')\n",
    "\n",
    "print(f'\\n=== ADVANCED METHODS EVALUATION COMPLETE ===')\n",
    "print(f'Best performing model: {best_advanced[\"Model\"]}')\n",
    "print(f'Achieved R² = {best_advanced[\"R² (Test)\"]:.4f}')\n",
    "print(f'Improvement over target: {best_advanced[\"R² (Test)\"] - target_r2:+.4f}')\n",
    "\n",
    "if best_advanced[\"R² (Test)\"] > target_r2:\n",
    "    print(f'SUCCESS! Advanced methods exceeded the target performance!')\n",
    "else:\n",
    "    print(f'Target not achieved. Consider further hyperparameter tuning.')\n",
    "\n",
    "print(f'\\nAll advanced models and artifacts saved to: {MODELS_DIR}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
